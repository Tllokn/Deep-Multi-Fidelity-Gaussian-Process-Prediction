{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WhiteNoiseKernel' from 'gpytorch.kernels' (c:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\gpytorch\\kernels\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-27105cdbf6cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRBFKernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScaleKernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWhiteNoiseKernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WhiteNoiseKernel' from 'gpytorch.kernels' (c:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\gpytorch\\kernels\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch.nn as nn\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31.1779, 29.1824, 26.4095, 26.9644, 30.0900, 26.8370, 25.2800, 31.4606,\n",
      "        30.1555])\n"
     ]
    }
   ],
   "source": [
    "XTest = np.loadtxt('./dataset/x_test_high.txt').reshape(-1,32)\n",
    "XTest = XTest.mean(axis=1).reshape(-1)\n",
    "YTest = np.loadtxt('./dataset/y_test_high.txt').reshape(-1)\n",
    "\n",
    "XLow = np.loadtxt('./dataset/x_train_low.txt').reshape(-1,32)\n",
    "XLow = XLow.mean(axis=1).reshape(-1)\n",
    "XHigh = np.loadtxt('./dataset/x_train_high.txt').reshape(-1,32)\n",
    "XHigh = XHigh.mean(axis=1).reshape(-1)\n",
    "YLow = (np.loadtxt('./dataset/y_train_low.txt')*1e4).reshape(-1)\n",
    "YHigh = np.loadtxt('./dataset/y_train_high.txt').reshape(-1)\n",
    "\n",
    "XLow = torch.from_numpy(XLow).float()\n",
    "YLow = torch.from_numpy(YLow).float()\n",
    "XHigh = torch.from_numpy(XHigh).float()\n",
    "YHigh = torch.from_numpy(YHigh).float()\n",
    "XTest = torch.from_numpy(XTest).float()\n",
    "YTest = torch.from_numpy(YTest).float()\n",
    "print(YTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "InDim=32\n",
    "HidDim=50\n",
    "OutDim=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMultiGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, InDim,HidDim,OutDim,XLow, YLow, XHigh,YHigh,likelihoodLow,likelihoodHigh):\n",
    "        super(DeepMultiGPModel, self).__init__(self, InDim,HidDim,OutDim,XLow, YLow, XHigh,YHigh,likelihoodLow,likelihoodHigh)\n",
    "        self.mean_module_Low = gpytorch.means.ConstantMean()\n",
    "        self.covar_module_Low = gpytorch.kernels.ScaleKernel(RBFKernel())\n",
    "        \n",
    "        self.mean_module_High = gpytorch.means.ConstantMean()\n",
    "        self.covar_module_High = gpytorch.kernels.ScaleKernel(RBFKernel())\n",
    "        \n",
    "        self.Linear1=nn.Linear(InDim,HidDim)\n",
    "        self.Linear2=nn.Linear(HidDim,OutDim)\n",
    "        self.ReLu=nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self,XLow, YLow, XHigh,YHigh):\n",
    "        Hidden1=self.Linear1(XLow)\n",
    "        ReLu1=self.ReLu(Hidden1)\n",
    "        Output1=self.Linear2(ReLu1)\n",
    "        Hidden2=self.Linear1(XHigh)\n",
    "        ReLu2=self.ReLu(Hidden2)\n",
    "        Output2=self.Linear2(ReLu2)\n",
    "        \n",
    "        mean_XLow = self.mean_module_Low(Output1)\n",
    "        covar_XLow = self.covar_module_Low(OutPut1)\n",
    "        Distri1=gpytorch.distributions.MultivariateNormal(mean_XLow, covar_XLow)\n",
    "        XHighNew = torch.stack([Output2, Distri1.mean])\n",
    "        mean_XHigh = self.mean_module_High(XHighNew)\n",
    "        covar_XHigh = self.covar_module_High(XHighNew)\n",
    "        Distri2=gpytorch.distributions.MultivariateNormal(mean_XHigh, covar_XHigh)\n",
    "        \n",
    "        return Distri2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 4 positional arguments but 11 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-afeed9fe868a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlikelihoodLow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianLikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlikelihoodHigh\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepMultiGPModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHidDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOutDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodLow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodHigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-1db6afed4169>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, InDim, HidDim, OutDim, XLow, YLow, XHigh, YHigh, likelihoodLow, likelihoodHigh)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDeepMultiGPModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExactGP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHidDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOutDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodLow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodHigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDeepMultiGPModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHidDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOutDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYLow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYHigh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodLow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlikelihoodHigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_module_Low\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConstantMean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovar_module_Low\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScaleKernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRBFKernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 4 positional arguments but 11 were given"
     ]
    }
   ],
   "source": [
    "likelihoodLow = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihoodHigh= gpytorch.likelihoods.noise_models\n",
    "model = DeepMultiGPModel(InDim,HidDim,OutDim,XLow, YLow, XHigh,YHigh,likelihoodLow,likelihoodHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10000 - Loss: 11.726\n",
      "Iter 11/10000 - Loss: 5.863\n",
      "Iter 21/10000 - Loss: 4.444\n",
      "Iter 31/10000 - Loss: 3.884\n",
      "Iter 41/10000 - Loss: 3.588\n",
      "Iter 51/10000 - Loss: 3.420\n",
      "Iter 61/10000 - Loss: 3.302\n",
      "Iter 71/10000 - Loss: 3.222\n",
      "Iter 81/10000 - Loss: 3.154\n",
      "Iter 91/10000 - Loss: 3.103\n",
      "Iter 101/10000 - Loss: 3.053\n",
      "Iter 111/10000 - Loss: 3.016\n",
      "Iter 121/10000 - Loss: 2.991\n",
      "Iter 131/10000 - Loss: 2.962\n",
      "Iter 141/10000 - Loss: 2.929\n",
      "Iter 151/10000 - Loss: 2.911\n",
      "Iter 161/10000 - Loss: 2.898\n",
      "Iter 171/10000 - Loss: 2.881\n",
      "Iter 181/10000 - Loss: 2.859\n",
      "Iter 191/10000 - Loss: 2.844\n",
      "Iter 201/10000 - Loss: 2.831\n",
      "Iter 211/10000 - Loss: 2.818\n",
      "Iter 221/10000 - Loss: 2.806\n",
      "Iter 231/10000 - Loss: 2.803\n",
      "Iter 241/10000 - Loss: 2.787\n",
      "Iter 251/10000 - Loss: 2.778\n",
      "Iter 261/10000 - Loss: 2.773\n",
      "Iter 271/10000 - Loss: 2.769\n",
      "Iter 281/10000 - Loss: 2.758\n",
      "Iter 291/10000 - Loss: 2.754\n",
      "Iter 301/10000 - Loss: 2.748\n",
      "Iter 311/10000 - Loss: 2.741\n",
      "Iter 321/10000 - Loss: 2.734\n",
      "Iter 331/10000 - Loss: 2.732\n",
      "Iter 341/10000 - Loss: 2.723\n",
      "Iter 351/10000 - Loss: 2.723\n",
      "Iter 361/10000 - Loss: 2.722\n",
      "Iter 371/10000 - Loss: 2.715\n",
      "Iter 381/10000 - Loss: 2.713\n",
      "Iter 391/10000 - Loss: 2.709\n",
      "Iter 401/10000 - Loss: 2.703\n",
      "Iter 411/10000 - Loss: 2.703\n",
      "Iter 421/10000 - Loss: 2.698\n",
      "Iter 431/10000 - Loss: 2.699\n",
      "Iter 441/10000 - Loss: 2.691\n",
      "Iter 451/10000 - Loss: 2.693\n",
      "Iter 461/10000 - Loss: 2.691\n",
      "Iter 471/10000 - Loss: 2.685\n",
      "Iter 481/10000 - Loss: 2.691\n",
      "Iter 491/10000 - Loss: 2.685\n",
      "Iter 501/10000 - Loss: 2.679\n",
      "Iter 511/10000 - Loss: 2.679\n",
      "Iter 521/10000 - Loss: 2.679\n",
      "Iter 531/10000 - Loss: 2.673\n",
      "Iter 541/10000 - Loss: 2.674\n",
      "Iter 551/10000 - Loss: 2.673\n",
      "Iter 561/10000 - Loss: 2.674\n",
      "Iter 571/10000 - Loss: 2.671\n",
      "Iter 581/10000 - Loss: 2.664\n",
      "Iter 591/10000 - Loss: 2.668\n",
      "Iter 601/10000 - Loss: 2.666\n",
      "Iter 611/10000 - Loss: 2.663\n",
      "Iter 621/10000 - Loss: 2.663\n",
      "Iter 631/10000 - Loss: 2.663\n",
      "Iter 641/10000 - Loss: 2.665\n",
      "Iter 651/10000 - Loss: 2.659\n",
      "Iter 661/10000 - Loss: 2.660\n",
      "Iter 671/10000 - Loss: 2.660\n",
      "Iter 681/10000 - Loss: 2.661\n",
      "Iter 691/10000 - Loss: 2.659\n",
      "Iter 701/10000 - Loss: 2.655\n",
      "Iter 711/10000 - Loss: 2.652\n",
      "Iter 721/10000 - Loss: 2.651\n",
      "Iter 731/10000 - Loss: 2.652\n",
      "Iter 741/10000 - Loss: 2.651\n",
      "Iter 751/10000 - Loss: 2.652\n",
      "Iter 761/10000 - Loss: 2.651\n",
      "Iter 771/10000 - Loss: 2.654\n",
      "Iter 781/10000 - Loss: 2.648\n",
      "Iter 791/10000 - Loss: 2.649\n",
      "Iter 801/10000 - Loss: 2.646\n",
      "Iter 811/10000 - Loss: 2.647\n",
      "Iter 821/10000 - Loss: 2.647\n",
      "Iter 831/10000 - Loss: 2.646\n",
      "Iter 841/10000 - Loss: 2.647\n",
      "Iter 851/10000 - Loss: 2.643\n",
      "Iter 861/10000 - Loss: 2.646\n",
      "Iter 871/10000 - Loss: 2.643\n",
      "Iter 881/10000 - Loss: 2.642\n",
      "Iter 891/10000 - Loss: 2.643\n",
      "Iter 901/10000 - Loss: 2.645\n",
      "Iter 911/10000 - Loss: 2.641\n",
      "Iter 921/10000 - Loss: 2.642\n",
      "Iter 931/10000 - Loss: 2.644\n",
      "Iter 941/10000 - Loss: 2.642\n",
      "Iter 951/10000 - Loss: 2.640\n",
      "Iter 961/10000 - Loss: 2.642\n",
      "Iter 971/10000 - Loss: 2.641\n",
      "Iter 981/10000 - Loss: 2.639\n",
      "Iter 991/10000 - Loss: 2.639\n",
      "Iter 1001/10000 - Loss: 2.639\n",
      "Iter 1011/10000 - Loss: 2.636\n",
      "Iter 1021/10000 - Loss: 2.640\n",
      "Iter 1031/10000 - Loss: 2.639\n",
      "Iter 1041/10000 - Loss: 2.640\n",
      "Iter 1051/10000 - Loss: 2.639\n",
      "Iter 1061/10000 - Loss: 2.638\n",
      "Iter 1071/10000 - Loss: 2.638\n",
      "Iter 1081/10000 - Loss: 2.637\n",
      "Iter 1091/10000 - Loss: 2.638\n",
      "Iter 1101/10000 - Loss: 2.637\n",
      "Iter 1111/10000 - Loss: 2.638\n",
      "Iter 1121/10000 - Loss: 2.636\n",
      "Iter 1131/10000 - Loss: 2.641\n",
      "Iter 1141/10000 - Loss: 2.638\n",
      "Iter 1151/10000 - Loss: 2.638\n",
      "Iter 1161/10000 - Loss: 2.632\n",
      "Iter 1171/10000 - Loss: 2.638\n",
      "Iter 1181/10000 - Loss: 2.634\n",
      "Iter 1191/10000 - Loss: 2.635\n",
      "Iter 1201/10000 - Loss: 2.633\n",
      "Iter 1211/10000 - Loss: 2.637\n",
      "Iter 1221/10000 - Loss: 2.635\n",
      "Iter 1231/10000 - Loss: 2.634\n",
      "Iter 1241/10000 - Loss: 2.633\n",
      "Iter 1251/10000 - Loss: 2.634\n",
      "Iter 1261/10000 - Loss: 2.637\n",
      "Iter 1271/10000 - Loss: 2.636\n",
      "Iter 1281/10000 - Loss: 2.636\n",
      "Iter 1291/10000 - Loss: 2.633\n",
      "Iter 1301/10000 - Loss: 2.636\n",
      "Iter 1311/10000 - Loss: 2.633\n",
      "Iter 1321/10000 - Loss: 2.633\n",
      "Iter 1331/10000 - Loss: 2.633\n",
      "Iter 1341/10000 - Loss: 2.633\n",
      "Iter 1351/10000 - Loss: 2.634\n",
      "Iter 1361/10000 - Loss: 2.631\n",
      "Iter 1371/10000 - Loss: 2.632\n",
      "Iter 1381/10000 - Loss: 2.634\n",
      "Iter 1391/10000 - Loss: 2.634\n",
      "Iter 1401/10000 - Loss: 2.634\n",
      "Iter 1411/10000 - Loss: 2.635\n",
      "Iter 1421/10000 - Loss: 2.633\n",
      "Iter 1431/10000 - Loss: 2.634\n",
      "Iter 1441/10000 - Loss: 2.631\n",
      "Iter 1451/10000 - Loss: 2.631\n",
      "Iter 1461/10000 - Loss: 2.630\n",
      "Iter 1471/10000 - Loss: 2.631\n",
      "Iter 1481/10000 - Loss: 2.633\n",
      "Iter 1491/10000 - Loss: 2.631\n",
      "Iter 1501/10000 - Loss: 2.633\n",
      "Iter 1511/10000 - Loss: 2.632\n",
      "Iter 1521/10000 - Loss: 2.637\n",
      "Iter 1531/10000 - Loss: 2.632\n",
      "Iter 1541/10000 - Loss: 2.630\n",
      "Iter 1551/10000 - Loss: 2.632\n",
      "Iter 1561/10000 - Loss: 2.634\n",
      "Iter 1571/10000 - Loss: 2.630\n",
      "Iter 1581/10000 - Loss: 2.629\n",
      "Iter 1591/10000 - Loss: 2.631\n",
      "Iter 1601/10000 - Loss: 2.632\n",
      "Iter 1611/10000 - Loss: 2.630\n",
      "Iter 1621/10000 - Loss: 2.631\n",
      "Iter 1631/10000 - Loss: 2.631\n",
      "Iter 1641/10000 - Loss: 2.628\n",
      "Iter 1651/10000 - Loss: 2.633\n",
      "Iter 1661/10000 - Loss: 2.631\n",
      "Iter 1671/10000 - Loss: 2.634\n",
      "Iter 1681/10000 - Loss: 2.631\n",
      "Iter 1691/10000 - Loss: 2.629\n",
      "Iter 1701/10000 - Loss: 2.633\n",
      "Iter 1711/10000 - Loss: 2.631\n",
      "Iter 1721/10000 - Loss: 2.631\n",
      "Iter 1731/10000 - Loss: 2.628\n",
      "Iter 1741/10000 - Loss: 2.631\n",
      "Iter 1751/10000 - Loss: 2.631\n",
      "Iter 1761/10000 - Loss: 2.629\n",
      "Iter 1771/10000 - Loss: 2.631\n",
      "Iter 1781/10000 - Loss: 2.630\n",
      "Iter 1791/10000 - Loss: 2.632\n",
      "Iter 1801/10000 - Loss: 2.627\n",
      "Iter 1811/10000 - Loss: 2.632\n",
      "Iter 1821/10000 - Loss: 2.628\n",
      "Iter 1831/10000 - Loss: 2.627\n",
      "Iter 1841/10000 - Loss: 2.628\n",
      "Iter 1851/10000 - Loss: 2.628\n",
      "Iter 1861/10000 - Loss: 2.632\n",
      "Iter 1871/10000 - Loss: 2.630\n",
      "Iter 1881/10000 - Loss: 2.629\n",
      "Iter 1891/10000 - Loss: 2.630\n",
      "Iter 1901/10000 - Loss: 2.628\n",
      "Iter 1911/10000 - Loss: 2.630\n",
      "Iter 1921/10000 - Loss: 2.629\n",
      "Iter 1931/10000 - Loss: 2.629\n",
      "Iter 1941/10000 - Loss: 2.627\n",
      "Iter 1951/10000 - Loss: 2.629\n",
      "Iter 1961/10000 - Loss: 2.630\n",
      "Iter 1971/10000 - Loss: 2.630\n",
      "Iter 1981/10000 - Loss: 2.629\n",
      "Iter 1991/10000 - Loss: 2.629\n",
      "Iter 2001/10000 - Loss: 2.629\n",
      "Iter 2011/10000 - Loss: 2.630\n",
      "Iter 2021/10000 - Loss: 2.628\n",
      "Iter 2031/10000 - Loss: 2.632\n",
      "Iter 2041/10000 - Loss: 2.633\n",
      "Iter 2051/10000 - Loss: 2.628\n",
      "Iter 2061/10000 - Loss: 2.629\n",
      "Iter 2071/10000 - Loss: 2.629\n",
      "Iter 2081/10000 - Loss: 2.628\n",
      "Iter 2091/10000 - Loss: 2.629\n",
      "Iter 2101/10000 - Loss: 2.627\n",
      "Iter 2111/10000 - Loss: 2.632\n",
      "Iter 2121/10000 - Loss: 2.630\n",
      "Iter 2131/10000 - Loss: 2.627\n",
      "Iter 2141/10000 - Loss: 2.631\n",
      "Iter 2151/10000 - Loss: 2.629\n",
      "Iter 2161/10000 - Loss: 2.628\n",
      "Iter 2171/10000 - Loss: 2.627\n",
      "Iter 2181/10000 - Loss: 2.629\n",
      "Iter 2191/10000 - Loss: 2.626\n",
      "Iter 2201/10000 - Loss: 2.631\n",
      "Iter 2211/10000 - Loss: 2.628\n",
      "Iter 2221/10000 - Loss: 2.627\n",
      "Iter 2231/10000 - Loss: 2.631\n",
      "Iter 2241/10000 - Loss: 2.632\n",
      "Iter 2251/10000 - Loss: 2.627\n",
      "Iter 2261/10000 - Loss: 2.628\n",
      "Iter 2271/10000 - Loss: 2.627\n",
      "Iter 2281/10000 - Loss: 2.626\n",
      "Iter 2291/10000 - Loss: 2.630\n",
      "Iter 2301/10000 - Loss: 2.631\n",
      "Iter 2311/10000 - Loss: 2.627\n",
      "Iter 2321/10000 - Loss: 2.627\n",
      "Iter 2331/10000 - Loss: 2.631\n",
      "Iter 2341/10000 - Loss: 2.627\n",
      "Iter 2351/10000 - Loss: 2.629\n",
      "Iter 2361/10000 - Loss: 2.629\n",
      "Iter 2371/10000 - Loss: 2.624\n",
      "Iter 2381/10000 - Loss: 2.625\n",
      "Iter 2391/10000 - Loss: 2.625\n",
      "Iter 2401/10000 - Loss: 2.625\n",
      "Iter 2411/10000 - Loss: 2.627\n",
      "Iter 2421/10000 - Loss: 2.625\n",
      "Iter 2431/10000 - Loss: 2.627\n",
      "Iter 2441/10000 - Loss: 2.626\n",
      "Iter 2451/10000 - Loss: 2.627\n",
      "Iter 2461/10000 - Loss: 2.630\n",
      "Iter 2471/10000 - Loss: 2.631\n",
      "Iter 2481/10000 - Loss: 2.625\n",
      "Iter 2491/10000 - Loss: 2.629\n",
      "Iter 2501/10000 - Loss: 2.629\n",
      "Iter 2511/10000 - Loss: 2.626\n",
      "Iter 2521/10000 - Loss: 2.630\n",
      "Iter 2531/10000 - Loss: 2.629\n",
      "Iter 2541/10000 - Loss: 2.625\n",
      "Iter 2551/10000 - Loss: 2.629\n",
      "Iter 2561/10000 - Loss: 2.625\n",
      "Iter 2571/10000 - Loss: 2.627\n",
      "Iter 2581/10000 - Loss: 2.630\n",
      "Iter 2591/10000 - Loss: 2.628\n",
      "Iter 2601/10000 - Loss: 2.627\n",
      "Iter 2611/10000 - Loss: 2.627\n",
      "Iter 2621/10000 - Loss: 2.628\n",
      "Iter 2631/10000 - Loss: 2.628\n",
      "Iter 2641/10000 - Loss: 2.631\n",
      "Iter 2651/10000 - Loss: 2.628\n",
      "Iter 2661/10000 - Loss: 2.626\n",
      "Iter 2671/10000 - Loss: 2.625\n",
      "Iter 2681/10000 - Loss: 2.631\n",
      "Iter 2691/10000 - Loss: 2.627\n",
      "Iter 2701/10000 - Loss: 2.629\n",
      "Iter 2711/10000 - Loss: 2.627\n",
      "Iter 2721/10000 - Loss: 2.628\n",
      "Iter 2731/10000 - Loss: 2.626\n",
      "Iter 2741/10000 - Loss: 2.626\n",
      "Iter 2751/10000 - Loss: 2.627\n",
      "Iter 2761/10000 - Loss: 2.627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2771/10000 - Loss: 2.630\n",
      "Iter 2781/10000 - Loss: 2.625\n",
      "Iter 2791/10000 - Loss: 2.628\n",
      "Iter 2801/10000 - Loss: 2.625\n",
      "Iter 2811/10000 - Loss: 2.625\n",
      "Iter 2821/10000 - Loss: 2.626\n",
      "Iter 2831/10000 - Loss: 2.629\n",
      "Iter 2841/10000 - Loss: 2.630\n",
      "Iter 2851/10000 - Loss: 2.627\n",
      "Iter 2861/10000 - Loss: 2.628\n",
      "Iter 2871/10000 - Loss: 2.625\n",
      "Iter 2881/10000 - Loss: 2.626\n",
      "Iter 2891/10000 - Loss: 2.626\n",
      "Iter 2901/10000 - Loss: 2.627\n",
      "Iter 2911/10000 - Loss: 2.626\n",
      "Iter 2921/10000 - Loss: 2.627\n",
      "Iter 2931/10000 - Loss: 2.629\n",
      "Iter 2941/10000 - Loss: 2.626\n",
      "Iter 2951/10000 - Loss: 2.626\n",
      "Iter 2961/10000 - Loss: 2.625\n",
      "Iter 2971/10000 - Loss: 2.624\n",
      "Iter 2981/10000 - Loss: 2.624\n",
      "Iter 2991/10000 - Loss: 2.627\n",
      "Iter 3001/10000 - Loss: 2.625\n",
      "Iter 3011/10000 - Loss: 2.626\n",
      "Iter 3021/10000 - Loss: 2.625\n",
      "Iter 3031/10000 - Loss: 2.629\n",
      "Iter 3041/10000 - Loss: 2.628\n",
      "Iter 3051/10000 - Loss: 2.630\n",
      "Iter 3061/10000 - Loss: 2.627\n",
      "Iter 3071/10000 - Loss: 2.630\n",
      "Iter 3081/10000 - Loss: 2.625\n",
      "Iter 3091/10000 - Loss: 2.626\n",
      "Iter 3101/10000 - Loss: 2.627\n",
      "Iter 3111/10000 - Loss: 2.626\n",
      "Iter 3121/10000 - Loss: 2.626\n",
      "Iter 3131/10000 - Loss: 2.629\n",
      "Iter 3141/10000 - Loss: 2.624\n",
      "Iter 3151/10000 - Loss: 2.629\n",
      "Iter 3161/10000 - Loss: 2.623\n",
      "Iter 3171/10000 - Loss: 2.627\n",
      "Iter 3181/10000 - Loss: 2.627\n",
      "Iter 3191/10000 - Loss: 2.625\n",
      "Iter 3201/10000 - Loss: 2.627\n",
      "Iter 3211/10000 - Loss: 2.626\n",
      "Iter 3221/10000 - Loss: 2.628\n",
      "Iter 3231/10000 - Loss: 2.628\n",
      "Iter 3241/10000 - Loss: 2.626\n",
      "Iter 3251/10000 - Loss: 2.628\n",
      "Iter 3261/10000 - Loss: 2.627\n",
      "Iter 3271/10000 - Loss: 2.625\n",
      "Iter 3281/10000 - Loss: 2.627\n",
      "Iter 3291/10000 - Loss: 2.630\n",
      "Iter 3301/10000 - Loss: 2.626\n",
      "Iter 3311/10000 - Loss: 2.625\n",
      "Iter 3321/10000 - Loss: 2.624\n",
      "Iter 3331/10000 - Loss: 2.627\n",
      "Iter 3341/10000 - Loss: 2.626\n",
      "Iter 3351/10000 - Loss: 2.623\n",
      "Iter 3361/10000 - Loss: 2.624\n",
      "Iter 3371/10000 - Loss: 2.624\n",
      "Iter 3381/10000 - Loss: 2.627\n",
      "Iter 3391/10000 - Loss: 2.623\n",
      "Iter 3401/10000 - Loss: 2.625\n",
      "Iter 3411/10000 - Loss: 2.628\n",
      "Iter 3421/10000 - Loss: 2.627\n",
      "Iter 3431/10000 - Loss: 2.629\n",
      "Iter 3441/10000 - Loss: 2.624\n",
      "Iter 3451/10000 - Loss: 2.627\n",
      "Iter 3461/10000 - Loss: 2.626\n",
      "Iter 3471/10000 - Loss: 2.626\n",
      "Iter 3481/10000 - Loss: 2.625\n",
      "Iter 3491/10000 - Loss: 2.625\n",
      "Iter 3501/10000 - Loss: 2.624\n",
      "Iter 3511/10000 - Loss: 2.626\n",
      "Iter 3521/10000 - Loss: 2.624\n",
      "Iter 3531/10000 - Loss: 2.624\n",
      "Iter 3541/10000 - Loss: 2.628\n",
      "Iter 3551/10000 - Loss: 2.625\n",
      "Iter 3561/10000 - Loss: 2.624\n",
      "Iter 3571/10000 - Loss: 2.625\n",
      "Iter 3581/10000 - Loss: 2.626\n",
      "Iter 3591/10000 - Loss: 2.627\n",
      "Iter 3601/10000 - Loss: 2.625\n",
      "Iter 3611/10000 - Loss: 2.626\n",
      "Iter 3621/10000 - Loss: 2.626\n",
      "Iter 3631/10000 - Loss: 2.628\n",
      "Iter 3641/10000 - Loss: 2.626\n",
      "Iter 3651/10000 - Loss: 2.625\n",
      "Iter 3661/10000 - Loss: 2.625\n",
      "Iter 3671/10000 - Loss: 2.627\n",
      "Iter 3681/10000 - Loss: 2.625\n",
      "Iter 3691/10000 - Loss: 2.627\n",
      "Iter 3701/10000 - Loss: 2.626\n",
      "Iter 3711/10000 - Loss: 2.629\n",
      "Iter 3721/10000 - Loss: 2.627\n",
      "Iter 3731/10000 - Loss: 2.623\n",
      "Iter 3741/10000 - Loss: 2.628\n",
      "Iter 3751/10000 - Loss: 2.624\n",
      "Iter 3761/10000 - Loss: 2.628\n",
      "Iter 3771/10000 - Loss: 2.626\n",
      "Iter 3781/10000 - Loss: 2.625\n",
      "Iter 3791/10000 - Loss: 2.629\n",
      "Iter 3801/10000 - Loss: 2.625\n",
      "Iter 3811/10000 - Loss: 2.624\n",
      "Iter 3821/10000 - Loss: 2.625\n",
      "Iter 3831/10000 - Loss: 2.626\n",
      "Iter 3841/10000 - Loss: 2.626\n",
      "Iter 3851/10000 - Loss: 2.626\n",
      "Iter 3861/10000 - Loss: 2.627\n",
      "Iter 3871/10000 - Loss: 2.624\n",
      "Iter 3881/10000 - Loss: 2.627\n",
      "Iter 3891/10000 - Loss: 2.624\n",
      "Iter 3901/10000 - Loss: 2.627\n",
      "Iter 3911/10000 - Loss: 2.624\n",
      "Iter 3921/10000 - Loss: 2.626\n",
      "Iter 3931/10000 - Loss: 2.626\n",
      "Iter 3941/10000 - Loss: 2.626\n",
      "Iter 3951/10000 - Loss: 2.625\n",
      "Iter 3961/10000 - Loss: 2.625\n",
      "Iter 3971/10000 - Loss: 2.626\n",
      "Iter 3981/10000 - Loss: 2.624\n",
      "Iter 3991/10000 - Loss: 2.626\n",
      "Iter 4001/10000 - Loss: 2.626\n",
      "Iter 4011/10000 - Loss: 2.624\n",
      "Iter 4021/10000 - Loss: 2.629\n",
      "Iter 4031/10000 - Loss: 2.626\n",
      "Iter 4041/10000 - Loss: 2.626\n",
      "Iter 4051/10000 - Loss: 2.627\n",
      "Iter 4061/10000 - Loss: 2.627\n",
      "Iter 4071/10000 - Loss: 2.624\n",
      "Iter 4081/10000 - Loss: 2.624\n",
      "Iter 4091/10000 - Loss: 2.626\n",
      "Iter 4101/10000 - Loss: 2.626\n",
      "Iter 4111/10000 - Loss: 2.627\n",
      "Iter 4121/10000 - Loss: 2.625\n",
      "Iter 4131/10000 - Loss: 2.627\n",
      "Iter 4141/10000 - Loss: 2.628\n",
      "Iter 4151/10000 - Loss: 2.629\n",
      "Iter 4161/10000 - Loss: 2.631\n",
      "Iter 4171/10000 - Loss: 2.625\n",
      "Iter 4181/10000 - Loss: 2.627\n",
      "Iter 4191/10000 - Loss: 2.629\n",
      "Iter 4201/10000 - Loss: 2.625\n",
      "Iter 4211/10000 - Loss: 2.626\n",
      "Iter 4221/10000 - Loss: 2.625\n",
      "Iter 4231/10000 - Loss: 2.629\n",
      "Iter 4241/10000 - Loss: 2.625\n",
      "Iter 4251/10000 - Loss: 2.623\n",
      "Iter 4261/10000 - Loss: 2.626\n",
      "Iter 4271/10000 - Loss: 2.626\n",
      "Iter 4281/10000 - Loss: 2.626\n",
      "Iter 4291/10000 - Loss: 2.627\n",
      "Iter 4301/10000 - Loss: 2.623\n",
      "Iter 4311/10000 - Loss: 2.628\n",
      "Iter 4321/10000 - Loss: 2.625\n",
      "Iter 4331/10000 - Loss: 2.626\n",
      "Iter 4341/10000 - Loss: 2.626\n",
      "Iter 4351/10000 - Loss: 2.628\n",
      "Iter 4361/10000 - Loss: 2.623\n",
      "Iter 4371/10000 - Loss: 2.625\n",
      "Iter 4381/10000 - Loss: 2.625\n",
      "Iter 4391/10000 - Loss: 2.628\n",
      "Iter 4401/10000 - Loss: 2.625\n",
      "Iter 4411/10000 - Loss: 2.625\n",
      "Iter 4421/10000 - Loss: 2.627\n",
      "Iter 4431/10000 - Loss: 2.627\n",
      "Iter 4441/10000 - Loss: 2.628\n",
      "Iter 4451/10000 - Loss: 2.622\n",
      "Iter 4461/10000 - Loss: 2.626\n",
      "Iter 4471/10000 - Loss: 2.626\n",
      "Iter 4481/10000 - Loss: 2.625\n",
      "Iter 4491/10000 - Loss: 2.625\n",
      "Iter 4501/10000 - Loss: 2.627\n",
      "Iter 4511/10000 - Loss: 2.623\n",
      "Iter 4521/10000 - Loss: 2.624\n",
      "Iter 4531/10000 - Loss: 2.627\n",
      "Iter 4541/10000 - Loss: 2.626\n",
      "Iter 4551/10000 - Loss: 2.628\n",
      "Iter 4561/10000 - Loss: 2.628\n",
      "Iter 4571/10000 - Loss: 2.625\n",
      "Iter 4581/10000 - Loss: 2.630\n",
      "Iter 4591/10000 - Loss: 2.627\n",
      "Iter 4601/10000 - Loss: 2.624\n",
      "Iter 4611/10000 - Loss: 2.624\n",
      "Iter 4621/10000 - Loss: 2.626\n",
      "Iter 4631/10000 - Loss: 2.624\n",
      "Iter 4641/10000 - Loss: 2.623\n",
      "Iter 4651/10000 - Loss: 2.625\n",
      "Iter 4661/10000 - Loss: 2.628\n",
      "Iter 4671/10000 - Loss: 2.630\n",
      "Iter 4681/10000 - Loss: 2.624\n",
      "Iter 4691/10000 - Loss: 2.627\n",
      "Iter 4701/10000 - Loss: 2.627\n",
      "Iter 4711/10000 - Loss: 2.626\n",
      "Iter 4721/10000 - Loss: 2.626\n",
      "Iter 4731/10000 - Loss: 2.624\n",
      "Iter 4741/10000 - Loss: 2.627\n",
      "Iter 4751/10000 - Loss: 2.627\n",
      "Iter 4761/10000 - Loss: 2.627\n",
      "Iter 4771/10000 - Loss: 2.628\n",
      "Iter 4781/10000 - Loss: 2.626\n",
      "Iter 4791/10000 - Loss: 2.627\n",
      "Iter 4801/10000 - Loss: 2.624\n",
      "Iter 4811/10000 - Loss: 2.626\n",
      "Iter 4821/10000 - Loss: 2.627\n",
      "Iter 4831/10000 - Loss: 2.625\n",
      "Iter 4841/10000 - Loss: 2.625\n",
      "Iter 4851/10000 - Loss: 2.628\n",
      "Iter 4861/10000 - Loss: 2.627\n",
      "Iter 4871/10000 - Loss: 2.625\n",
      "Iter 4881/10000 - Loss: 2.625\n",
      "Iter 4891/10000 - Loss: 2.624\n",
      "Iter 4901/10000 - Loss: 2.625\n",
      "Iter 4911/10000 - Loss: 2.624\n",
      "Iter 4921/10000 - Loss: 2.627\n",
      "Iter 4931/10000 - Loss: 2.630\n",
      "Iter 4941/10000 - Loss: 2.625\n",
      "Iter 4951/10000 - Loss: 2.626\n",
      "Iter 4961/10000 - Loss: 2.627\n",
      "Iter 4971/10000 - Loss: 2.628\n",
      "Iter 4981/10000 - Loss: 2.627\n",
      "Iter 4991/10000 - Loss: 2.625\n",
      "Iter 5001/10000 - Loss: 2.624\n",
      "Iter 5011/10000 - Loss: 2.624\n",
      "Iter 5021/10000 - Loss: 2.629\n",
      "Iter 5031/10000 - Loss: 2.628\n",
      "Iter 5041/10000 - Loss: 2.623\n",
      "Iter 5051/10000 - Loss: 2.626\n",
      "Iter 5061/10000 - Loss: 2.626\n",
      "Iter 5071/10000 - Loss: 2.626\n",
      "Iter 5081/10000 - Loss: 2.629\n",
      "Iter 5091/10000 - Loss: 2.628\n",
      "Iter 5101/10000 - Loss: 2.626\n",
      "Iter 5111/10000 - Loss: 2.625\n",
      "Iter 5121/10000 - Loss: 2.626\n",
      "Iter 5131/10000 - Loss: 2.627\n",
      "Iter 5141/10000 - Loss: 2.627\n",
      "Iter 5151/10000 - Loss: 2.628\n",
      "Iter 5161/10000 - Loss: 2.628\n",
      "Iter 5171/10000 - Loss: 2.624\n",
      "Iter 5181/10000 - Loss: 2.627\n",
      "Iter 5191/10000 - Loss: 2.626\n",
      "Iter 5201/10000 - Loss: 2.625\n",
      "Iter 5211/10000 - Loss: 2.627\n",
      "Iter 5221/10000 - Loss: 2.628\n",
      "Iter 5231/10000 - Loss: 2.624\n",
      "Iter 5241/10000 - Loss: 2.626\n",
      "Iter 5251/10000 - Loss: 2.625\n",
      "Iter 5261/10000 - Loss: 2.629\n",
      "Iter 5271/10000 - Loss: 2.630\n",
      "Iter 5281/10000 - Loss: 2.629\n",
      "Iter 5291/10000 - Loss: 2.627\n",
      "Iter 5301/10000 - Loss: 2.627\n",
      "Iter 5311/10000 - Loss: 2.626\n",
      "Iter 5321/10000 - Loss: 2.629\n",
      "Iter 5331/10000 - Loss: 2.624\n",
      "Iter 5341/10000 - Loss: 2.625\n",
      "Iter 5351/10000 - Loss: 2.626\n",
      "Iter 5361/10000 - Loss: 2.624\n",
      "Iter 5371/10000 - Loss: 2.632\n",
      "Iter 5381/10000 - Loss: 2.628\n",
      "Iter 5391/10000 - Loss: 2.626\n",
      "Iter 5401/10000 - Loss: 2.625\n",
      "Iter 5411/10000 - Loss: 2.625\n",
      "Iter 5421/10000 - Loss: 2.625\n",
      "Iter 5431/10000 - Loss: 2.625\n",
      "Iter 5441/10000 - Loss: 2.625\n",
      "Iter 5451/10000 - Loss: 2.624\n",
      "Iter 5461/10000 - Loss: 2.625\n",
      "Iter 5471/10000 - Loss: 2.627\n",
      "Iter 5481/10000 - Loss: 2.628\n",
      "Iter 5491/10000 - Loss: 2.625\n",
      "Iter 5501/10000 - Loss: 2.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5511/10000 - Loss: 2.626\n",
      "Iter 5521/10000 - Loss: 2.627\n",
      "Iter 5531/10000 - Loss: 2.623\n",
      "Iter 5541/10000 - Loss: 2.623\n",
      "Iter 5551/10000 - Loss: 2.625\n",
      "Iter 5561/10000 - Loss: 2.626\n",
      "Iter 5571/10000 - Loss: 2.625\n",
      "Iter 5581/10000 - Loss: 2.625\n",
      "Iter 5591/10000 - Loss: 2.623\n",
      "Iter 5601/10000 - Loss: 2.623\n",
      "Iter 5611/10000 - Loss: 2.625\n",
      "Iter 5621/10000 - Loss: 2.625\n",
      "Iter 5631/10000 - Loss: 2.623\n",
      "Iter 5641/10000 - Loss: 2.624\n",
      "Iter 5651/10000 - Loss: 2.623\n",
      "Iter 5661/10000 - Loss: 2.626\n",
      "Iter 5671/10000 - Loss: 2.627\n",
      "Iter 5681/10000 - Loss: 2.626\n",
      "Iter 5691/10000 - Loss: 2.627\n",
      "Iter 5701/10000 - Loss: 2.627\n",
      "Iter 5711/10000 - Loss: 2.624\n",
      "Iter 5721/10000 - Loss: 2.624\n",
      "Iter 5731/10000 - Loss: 2.625\n",
      "Iter 5741/10000 - Loss: 2.626\n",
      "Iter 5751/10000 - Loss: 2.626\n",
      "Iter 5761/10000 - Loss: 2.625\n",
      "Iter 5771/10000 - Loss: 2.627\n",
      "Iter 5781/10000 - Loss: 2.625\n",
      "Iter 5791/10000 - Loss: 2.628\n",
      "Iter 5801/10000 - Loss: 2.625\n",
      "Iter 5811/10000 - Loss: 2.626\n",
      "Iter 5821/10000 - Loss: 2.626\n",
      "Iter 5831/10000 - Loss: 2.626\n",
      "Iter 5841/10000 - Loss: 2.626\n",
      "Iter 5851/10000 - Loss: 2.623\n",
      "Iter 5861/10000 - Loss: 2.627\n",
      "Iter 5871/10000 - Loss: 2.625\n",
      "Iter 5881/10000 - Loss: 2.626\n",
      "Iter 5891/10000 - Loss: 2.625\n",
      "Iter 5901/10000 - Loss: 2.630\n",
      "Iter 5911/10000 - Loss: 2.623\n",
      "Iter 5921/10000 - Loss: 2.624\n",
      "Iter 5931/10000 - Loss: 2.628\n",
      "Iter 5941/10000 - Loss: 2.627\n",
      "Iter 5951/10000 - Loss: 2.624\n",
      "Iter 5961/10000 - Loss: 2.623\n",
      "Iter 5971/10000 - Loss: 2.625\n",
      "Iter 5981/10000 - Loss: 2.626\n",
      "Iter 5991/10000 - Loss: 2.624\n",
      "Iter 6001/10000 - Loss: 2.627\n",
      "Iter 6011/10000 - Loss: 2.626\n",
      "Iter 6021/10000 - Loss: 2.627\n",
      "Iter 6031/10000 - Loss: 2.625\n",
      "Iter 6041/10000 - Loss: 2.625\n",
      "Iter 6051/10000 - Loss: 2.626\n",
      "Iter 6061/10000 - Loss: 2.625\n",
      "Iter 6071/10000 - Loss: 2.628\n",
      "Iter 6081/10000 - Loss: 2.624\n",
      "Iter 6091/10000 - Loss: 2.628\n",
      "Iter 6101/10000 - Loss: 2.626\n",
      "Iter 6111/10000 - Loss: 2.628\n",
      "Iter 6121/10000 - Loss: 2.625\n",
      "Iter 6131/10000 - Loss: 2.630\n",
      "Iter 6141/10000 - Loss: 2.629\n",
      "Iter 6151/10000 - Loss: 2.627\n",
      "Iter 6161/10000 - Loss: 2.625\n",
      "Iter 6171/10000 - Loss: 2.624\n",
      "Iter 6181/10000 - Loss: 2.627\n",
      "Iter 6191/10000 - Loss: 2.626\n",
      "Iter 6201/10000 - Loss: 2.626\n",
      "Iter 6211/10000 - Loss: 2.625\n",
      "Iter 6221/10000 - Loss: 2.627\n",
      "Iter 6231/10000 - Loss: 2.626\n",
      "Iter 6241/10000 - Loss: 2.626\n",
      "Iter 6251/10000 - Loss: 2.627\n",
      "Iter 6261/10000 - Loss: 2.625\n",
      "Iter 6271/10000 - Loss: 2.626\n",
      "Iter 6281/10000 - Loss: 2.626\n",
      "Iter 6291/10000 - Loss: 2.627\n",
      "Iter 6301/10000 - Loss: 2.626\n",
      "Iter 6311/10000 - Loss: 2.626\n",
      "Iter 6321/10000 - Loss: 2.624\n",
      "Iter 6331/10000 - Loss: 2.626\n",
      "Iter 6341/10000 - Loss: 2.626\n",
      "Iter 6351/10000 - Loss: 2.625\n",
      "Iter 6361/10000 - Loss: 2.625\n",
      "Iter 6371/10000 - Loss: 2.628\n",
      "Iter 6381/10000 - Loss: 2.626\n",
      "Iter 6391/10000 - Loss: 2.628\n",
      "Iter 6401/10000 - Loss: 2.627\n",
      "Iter 6411/10000 - Loss: 2.626\n",
      "Iter 6421/10000 - Loss: 2.625\n",
      "Iter 6431/10000 - Loss: 2.629\n",
      "Iter 6441/10000 - Loss: 2.632\n",
      "Iter 6451/10000 - Loss: 2.627\n",
      "Iter 6461/10000 - Loss: 2.626\n",
      "Iter 6471/10000 - Loss: 2.626\n",
      "Iter 6481/10000 - Loss: 2.628\n",
      "Iter 6491/10000 - Loss: 2.629\n",
      "Iter 6501/10000 - Loss: 2.628\n",
      "Iter 6511/10000 - Loss: 2.625\n",
      "Iter 6521/10000 - Loss: 2.627\n",
      "Iter 6531/10000 - Loss: 2.626\n",
      "Iter 6541/10000 - Loss: 2.627\n",
      "Iter 6551/10000 - Loss: 2.627\n",
      "Iter 6561/10000 - Loss: 2.628\n",
      "Iter 6571/10000 - Loss: 2.625\n",
      "Iter 6581/10000 - Loss: 2.624\n",
      "Iter 6591/10000 - Loss: 2.628\n",
      "Iter 6601/10000 - Loss: 2.624\n",
      "Iter 6611/10000 - Loss: 2.625\n",
      "Iter 6621/10000 - Loss: 2.625\n",
      "Iter 6631/10000 - Loss: 2.625\n",
      "Iter 6641/10000 - Loss: 2.625\n",
      "Iter 6651/10000 - Loss: 2.627\n",
      "Iter 6661/10000 - Loss: 2.627\n",
      "Iter 6671/10000 - Loss: 2.627\n",
      "Iter 6681/10000 - Loss: 2.631\n",
      "Iter 6691/10000 - Loss: 2.627\n",
      "Iter 6701/10000 - Loss: 2.627\n",
      "Iter 6711/10000 - Loss: 2.627\n",
      "Iter 6721/10000 - Loss: 2.627\n",
      "Iter 6731/10000 - Loss: 2.630\n",
      "Iter 6741/10000 - Loss: 2.626\n",
      "Iter 6751/10000 - Loss: 2.627\n",
      "Iter 6761/10000 - Loss: 2.625\n",
      "Iter 6771/10000 - Loss: 2.626\n",
      "Iter 6781/10000 - Loss: 2.627\n",
      "Iter 6791/10000 - Loss: 2.625\n",
      "Iter 6801/10000 - Loss: 2.624\n",
      "Iter 6811/10000 - Loss: 2.625\n",
      "Iter 6821/10000 - Loss: 2.627\n",
      "Iter 6831/10000 - Loss: 2.627\n",
      "Iter 6841/10000 - Loss: 2.624\n",
      "Iter 6851/10000 - Loss: 2.627\n",
      "Iter 6861/10000 - Loss: 2.626\n",
      "Iter 6871/10000 - Loss: 2.627\n",
      "Iter 6881/10000 - Loss: 2.625\n",
      "Iter 6891/10000 - Loss: 2.626\n",
      "Iter 6901/10000 - Loss: 2.623\n",
      "Iter 6911/10000 - Loss: 2.626\n",
      "Iter 6921/10000 - Loss: 2.628\n",
      "Iter 6931/10000 - Loss: 2.624\n",
      "Iter 6941/10000 - Loss: 2.625\n",
      "Iter 6951/10000 - Loss: 2.627\n",
      "Iter 6961/10000 - Loss: 2.628\n",
      "Iter 6971/10000 - Loss: 2.626\n",
      "Iter 6981/10000 - Loss: 2.626\n",
      "Iter 6991/10000 - Loss: 2.624\n",
      "Iter 7001/10000 - Loss: 2.627\n",
      "Iter 7011/10000 - Loss: 2.624\n",
      "Iter 7021/10000 - Loss: 2.624\n",
      "Iter 7031/10000 - Loss: 2.629\n",
      "Iter 7041/10000 - Loss: 2.627\n",
      "Iter 7051/10000 - Loss: 2.626\n",
      "Iter 7061/10000 - Loss: 2.628\n",
      "Iter 7071/10000 - Loss: 2.626\n",
      "Iter 7081/10000 - Loss: 2.624\n",
      "Iter 7091/10000 - Loss: 2.624\n",
      "Iter 7101/10000 - Loss: 2.623\n",
      "Iter 7111/10000 - Loss: 2.624\n",
      "Iter 7121/10000 - Loss: 2.626\n",
      "Iter 7131/10000 - Loss: 2.626\n",
      "Iter 7141/10000 - Loss: 2.625\n",
      "Iter 7151/10000 - Loss: 2.625\n",
      "Iter 7161/10000 - Loss: 2.628\n",
      "Iter 7171/10000 - Loss: 2.627\n",
      "Iter 7181/10000 - Loss: 2.625\n",
      "Iter 7191/10000 - Loss: 2.628\n",
      "Iter 7201/10000 - Loss: 2.626\n",
      "Iter 7211/10000 - Loss: 2.627\n",
      "Iter 7221/10000 - Loss: 2.628\n",
      "Iter 7231/10000 - Loss: 2.623\n",
      "Iter 7241/10000 - Loss: 2.626\n",
      "Iter 7251/10000 - Loss: 2.627\n",
      "Iter 7261/10000 - Loss: 2.624\n",
      "Iter 7271/10000 - Loss: 2.623\n",
      "Iter 7281/10000 - Loss: 2.628\n",
      "Iter 7291/10000 - Loss: 2.625\n",
      "Iter 7301/10000 - Loss: 2.627\n",
      "Iter 7311/10000 - Loss: 2.625\n",
      "Iter 7321/10000 - Loss: 2.624\n",
      "Iter 7331/10000 - Loss: 2.626\n",
      "Iter 7341/10000 - Loss: 2.626\n",
      "Iter 7351/10000 - Loss: 2.629\n",
      "Iter 7361/10000 - Loss: 2.626\n",
      "Iter 7371/10000 - Loss: 2.624\n",
      "Iter 7381/10000 - Loss: 2.627\n",
      "Iter 7391/10000 - Loss: 2.626\n",
      "Iter 7401/10000 - Loss: 2.624\n",
      "Iter 7411/10000 - Loss: 2.624\n",
      "Iter 7421/10000 - Loss: 2.625\n",
      "Iter 7431/10000 - Loss: 2.625\n",
      "Iter 7441/10000 - Loss: 2.625\n",
      "Iter 7451/10000 - Loss: 2.625\n",
      "Iter 7461/10000 - Loss: 2.625\n",
      "Iter 7471/10000 - Loss: 2.625\n",
      "Iter 7481/10000 - Loss: 2.625\n",
      "Iter 7491/10000 - Loss: 2.624\n",
      "Iter 7501/10000 - Loss: 2.627\n",
      "Iter 7511/10000 - Loss: 2.629\n",
      "Iter 7521/10000 - Loss: 2.623\n",
      "Iter 7531/10000 - Loss: 2.623\n",
      "Iter 7541/10000 - Loss: 2.623\n",
      "Iter 7551/10000 - Loss: 2.624\n",
      "Iter 7561/10000 - Loss: 2.626\n",
      "Iter 7571/10000 - Loss: 2.623\n",
      "Iter 7581/10000 - Loss: 2.625\n",
      "Iter 7591/10000 - Loss: 2.627\n",
      "Iter 7601/10000 - Loss: 2.626\n",
      "Iter 7611/10000 - Loss: 2.626\n",
      "Iter 7621/10000 - Loss: 2.626\n",
      "Iter 7631/10000 - Loss: 2.627\n",
      "Iter 7641/10000 - Loss: 2.627\n",
      "Iter 7651/10000 - Loss: 2.624\n",
      "Iter 7661/10000 - Loss: 2.624\n",
      "Iter 7671/10000 - Loss: 2.624\n",
      "Iter 7681/10000 - Loss: 2.625\n",
      "Iter 7691/10000 - Loss: 2.626\n",
      "Iter 7701/10000 - Loss: 2.624\n",
      "Iter 7711/10000 - Loss: 2.624\n",
      "Iter 7721/10000 - Loss: 2.624\n",
      "Iter 7731/10000 - Loss: 2.625\n",
      "Iter 7741/10000 - Loss: 2.625\n",
      "Iter 7751/10000 - Loss: 2.625\n",
      "Iter 7761/10000 - Loss: 2.623\n",
      "Iter 7771/10000 - Loss: 2.626\n",
      "Iter 7781/10000 - Loss: 2.627\n",
      "Iter 7791/10000 - Loss: 2.626\n",
      "Iter 7801/10000 - Loss: 2.623\n",
      "Iter 7811/10000 - Loss: 2.624\n",
      "Iter 7821/10000 - Loss: 2.624\n",
      "Iter 7831/10000 - Loss: 2.622\n",
      "Iter 7841/10000 - Loss: 2.624\n",
      "Iter 7851/10000 - Loss: 2.623\n",
      "Iter 7861/10000 - Loss: 2.624\n",
      "Iter 7871/10000 - Loss: 2.622\n",
      "Iter 7881/10000 - Loss: 2.624\n",
      "Iter 7891/10000 - Loss: 2.622\n",
      "Iter 7901/10000 - Loss: 2.623\n",
      "Iter 7911/10000 - Loss: 2.624\n",
      "Iter 7921/10000 - Loss: 2.624\n",
      "Iter 7931/10000 - Loss: 2.625\n",
      "Iter 7941/10000 - Loss: 2.623\n",
      "Iter 7951/10000 - Loss: 2.623\n",
      "Iter 7961/10000 - Loss: 2.624\n",
      "Iter 7971/10000 - Loss: 2.622\n",
      "Iter 7981/10000 - Loss: 2.623\n",
      "Iter 7991/10000 - Loss: 2.623\n",
      "Iter 8001/10000 - Loss: 2.623\n",
      "Iter 8011/10000 - Loss: 2.623\n",
      "Iter 8021/10000 - Loss: 2.623\n",
      "Iter 8031/10000 - Loss: 2.624\n",
      "Iter 8041/10000 - Loss: 2.623\n",
      "Iter 8051/10000 - Loss: 2.622\n",
      "Iter 8061/10000 - Loss: 2.623\n",
      "Iter 8071/10000 - Loss: 2.626\n",
      "Iter 8081/10000 - Loss: 2.623\n",
      "Iter 8091/10000 - Loss: 2.622\n",
      "Iter 8101/10000 - Loss: 2.622\n",
      "Iter 8111/10000 - Loss: 2.624\n",
      "Iter 8121/10000 - Loss: 2.622\n",
      "Iter 8131/10000 - Loss: 2.623\n",
      "Iter 8141/10000 - Loss: 2.623\n",
      "Iter 8151/10000 - Loss: 2.624\n",
      "Iter 8161/10000 - Loss: 2.623\n",
      "Iter 8171/10000 - Loss: 2.623\n",
      "Iter 8181/10000 - Loss: 2.625\n",
      "Iter 8191/10000 - Loss: 2.625\n",
      "Iter 8201/10000 - Loss: 2.623\n",
      "Iter 8211/10000 - Loss: 2.624\n",
      "Iter 8221/10000 - Loss: 2.624\n",
      "Iter 8231/10000 - Loss: 2.621\n",
      "Iter 8241/10000 - Loss: 2.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8251/10000 - Loss: 2.624\n",
      "Iter 8261/10000 - Loss: 2.623\n",
      "Iter 8271/10000 - Loss: 2.623\n",
      "Iter 8281/10000 - Loss: 2.623\n",
      "Iter 8291/10000 - Loss: 2.622\n",
      "Iter 8301/10000 - Loss: 2.623\n",
      "Iter 8311/10000 - Loss: 2.623\n",
      "Iter 8321/10000 - Loss: 2.624\n",
      "Iter 8331/10000 - Loss: 2.623\n",
      "Iter 8341/10000 - Loss: 2.625\n",
      "Iter 8351/10000 - Loss: 2.623\n",
      "Iter 8361/10000 - Loss: 2.625\n",
      "Iter 8371/10000 - Loss: 2.622\n",
      "Iter 8381/10000 - Loss: 2.623\n",
      "Iter 8391/10000 - Loss: 2.622\n",
      "Iter 8401/10000 - Loss: 2.623\n",
      "Iter 8411/10000 - Loss: 2.623\n",
      "Iter 8421/10000 - Loss: 2.621\n",
      "Iter 8431/10000 - Loss: 2.624\n",
      "Iter 8441/10000 - Loss: 2.623\n",
      "Iter 8451/10000 - Loss: 2.624\n",
      "Iter 8461/10000 - Loss: 2.624\n",
      "Iter 8471/10000 - Loss: 2.624\n",
      "Iter 8481/10000 - Loss: 2.625\n",
      "Iter 8491/10000 - Loss: 2.624\n",
      "Iter 8501/10000 - Loss: 2.623\n",
      "Iter 8511/10000 - Loss: 2.624\n",
      "Iter 8521/10000 - Loss: 2.623\n",
      "Iter 8531/10000 - Loss: 2.624\n",
      "Iter 8541/10000 - Loss: 2.623\n",
      "Iter 8551/10000 - Loss: 2.623\n",
      "Iter 8561/10000 - Loss: 2.623\n",
      "Iter 8571/10000 - Loss: 2.622\n",
      "Iter 8581/10000 - Loss: 2.624\n",
      "Iter 8591/10000 - Loss: 2.623\n",
      "Iter 8601/10000 - Loss: 2.623\n",
      "Iter 8611/10000 - Loss: 2.624\n",
      "Iter 8621/10000 - Loss: 2.625\n",
      "Iter 8631/10000 - Loss: 2.623\n",
      "Iter 8641/10000 - Loss: 2.625\n",
      "Iter 8651/10000 - Loss: 2.623\n",
      "Iter 8661/10000 - Loss: 2.624\n",
      "Iter 8671/10000 - Loss: 2.624\n",
      "Iter 8681/10000 - Loss: 2.623\n",
      "Iter 8691/10000 - Loss: 2.622\n",
      "Iter 8701/10000 - Loss: 2.622\n",
      "Iter 8711/10000 - Loss: 2.624\n",
      "Iter 8721/10000 - Loss: 2.624\n",
      "Iter 8731/10000 - Loss: 2.622\n",
      "Iter 8741/10000 - Loss: 2.621\n",
      "Iter 8751/10000 - Loss: 2.622\n",
      "Iter 8761/10000 - Loss: 2.623\n",
      "Iter 8771/10000 - Loss: 2.625\n",
      "Iter 8781/10000 - Loss: 2.622\n",
      "Iter 8791/10000 - Loss: 2.623\n",
      "Iter 8801/10000 - Loss: 2.624\n",
      "Iter 8811/10000 - Loss: 2.624\n",
      "Iter 8821/10000 - Loss: 2.624\n",
      "Iter 8831/10000 - Loss: 2.624\n",
      "Iter 8841/10000 - Loss: 2.625\n",
      "Iter 8851/10000 - Loss: 2.623\n",
      "Iter 8861/10000 - Loss: 2.624\n",
      "Iter 8871/10000 - Loss: 2.623\n",
      "Iter 8881/10000 - Loss: 2.625\n",
      "Iter 8891/10000 - Loss: 2.623\n",
      "Iter 8901/10000 - Loss: 2.624\n",
      "Iter 8911/10000 - Loss: 2.623\n",
      "Iter 8921/10000 - Loss: 2.623\n",
      "Iter 8931/10000 - Loss: 2.625\n",
      "Iter 8941/10000 - Loss: 2.623\n",
      "Iter 8951/10000 - Loss: 2.626\n",
      "Iter 8961/10000 - Loss: 2.623\n",
      "Iter 8971/10000 - Loss: 2.623\n",
      "Iter 8981/10000 - Loss: 2.623\n",
      "Iter 8991/10000 - Loss: 2.624\n",
      "Iter 9001/10000 - Loss: 2.623\n",
      "Iter 9011/10000 - Loss: 2.625\n",
      "Iter 9021/10000 - Loss: 2.623\n",
      "Iter 9031/10000 - Loss: 2.624\n",
      "Iter 9041/10000 - Loss: 2.623\n",
      "Iter 9051/10000 - Loss: 2.624\n",
      "Iter 9061/10000 - Loss: 2.622\n",
      "Iter 9071/10000 - Loss: 2.623\n",
      "Iter 9081/10000 - Loss: 2.624\n",
      "Iter 9091/10000 - Loss: 2.623\n",
      "Iter 9101/10000 - Loss: 2.624\n",
      "Iter 9111/10000 - Loss: 2.623\n",
      "Iter 9121/10000 - Loss: 2.623\n",
      "Iter 9131/10000 - Loss: 2.627\n",
      "Iter 9141/10000 - Loss: 2.624\n",
      "Iter 9151/10000 - Loss: 2.623\n",
      "Iter 9161/10000 - Loss: 2.623\n",
      "Iter 9171/10000 - Loss: 2.621\n",
      "Iter 9181/10000 - Loss: 2.623\n",
      "Iter 9191/10000 - Loss: 2.624\n",
      "Iter 9201/10000 - Loss: 2.623\n",
      "Iter 9211/10000 - Loss: 2.623\n",
      "Iter 9221/10000 - Loss: 2.622\n",
      "Iter 9231/10000 - Loss: 2.623\n",
      "Iter 9241/10000 - Loss: 2.622\n",
      "Iter 9251/10000 - Loss: 2.622\n",
      "Iter 9261/10000 - Loss: 2.622\n",
      "Iter 9271/10000 - Loss: 2.622\n",
      "Iter 9281/10000 - Loss: 2.622\n",
      "Iter 9291/10000 - Loss: 2.623\n",
      "Iter 9301/10000 - Loss: 2.623\n",
      "Iter 9311/10000 - Loss: 2.622\n",
      "Iter 9321/10000 - Loss: 2.624\n",
      "Iter 9331/10000 - Loss: 2.622\n",
      "Iter 9341/10000 - Loss: 2.623\n",
      "Iter 9351/10000 - Loss: 2.624\n",
      "Iter 9361/10000 - Loss: 2.624\n",
      "Iter 9371/10000 - Loss: 2.624\n",
      "Iter 9381/10000 - Loss: 2.623\n",
      "Iter 9391/10000 - Loss: 2.623\n",
      "Iter 9401/10000 - Loss: 2.625\n",
      "Iter 9411/10000 - Loss: 2.623\n",
      "Iter 9421/10000 - Loss: 2.623\n",
      "Iter 9431/10000 - Loss: 2.622\n",
      "Iter 9441/10000 - Loss: 2.627\n",
      "Iter 9451/10000 - Loss: 2.623\n",
      "Iter 9461/10000 - Loss: 2.623\n",
      "Iter 9471/10000 - Loss: 2.624\n",
      "Iter 9481/10000 - Loss: 2.622\n",
      "Iter 9491/10000 - Loss: 2.621\n",
      "Iter 9501/10000 - Loss: 2.622\n",
      "Iter 9511/10000 - Loss: 2.623\n",
      "Iter 9521/10000 - Loss: 2.623\n",
      "Iter 9531/10000 - Loss: 2.623\n",
      "Iter 9541/10000 - Loss: 2.623\n",
      "Iter 9551/10000 - Loss: 2.626\n",
      "Iter 9561/10000 - Loss: 2.622\n",
      "Iter 9571/10000 - Loss: 2.622\n",
      "Iter 9581/10000 - Loss: 2.622\n",
      "Iter 9591/10000 - Loss: 2.621\n",
      "Iter 9601/10000 - Loss: 2.624\n",
      "Iter 9611/10000 - Loss: 2.622\n",
      "Iter 9621/10000 - Loss: 2.623\n",
      "Iter 9631/10000 - Loss: 2.623\n",
      "Iter 9641/10000 - Loss: 2.622\n",
      "Iter 9651/10000 - Loss: 2.622\n",
      "Iter 9661/10000 - Loss: 2.623\n",
      "Iter 9671/10000 - Loss: 2.625\n",
      "Iter 9681/10000 - Loss: 2.624\n",
      "Iter 9691/10000 - Loss: 2.623\n",
      "Iter 9701/10000 - Loss: 2.623\n",
      "Iter 9711/10000 - Loss: 2.623\n",
      "Iter 9721/10000 - Loss: 2.621\n",
      "Iter 9731/10000 - Loss: 2.623\n",
      "Iter 9741/10000 - Loss: 2.624\n",
      "Iter 9751/10000 - Loss: 2.624\n",
      "Iter 9761/10000 - Loss: 2.626\n",
      "Iter 9771/10000 - Loss: 2.622\n",
      "Iter 9781/10000 - Loss: 2.622\n",
      "Iter 9791/10000 - Loss: 2.622\n",
      "Iter 9801/10000 - Loss: 2.624\n",
      "Iter 9811/10000 - Loss: 2.624\n",
      "Iter 9821/10000 - Loss: 2.622\n",
      "Iter 9831/10000 - Loss: 2.624\n",
      "Iter 9841/10000 - Loss: 2.625\n",
      "Iter 9851/10000 - Loss: 2.625\n",
      "Iter 9861/10000 - Loss: 2.623\n",
      "Iter 9871/10000 - Loss: 2.623\n",
      "Iter 9881/10000 - Loss: 2.623\n",
      "Iter 9891/10000 - Loss: 2.623\n",
      "Iter 9901/10000 - Loss: 2.625\n",
      "Iter 9911/10000 - Loss: 2.622\n",
      "Iter 9921/10000 - Loss: 2.623\n",
      "Iter 9931/10000 - Loss: 2.625\n",
      "Iter 9941/10000 - Loss: 2.622\n",
      "Iter 9951/10000 - Loss: 2.624\n",
      "Iter 9961/10000 - Loss: 2.624\n",
      "Iter 9971/10000 - Loss: 2.623\n",
      "Iter 9981/10000 - Loss: 2.623\n",
      "Iter 9991/10000 - Loss: 2.622\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "modelLow.train()\n",
    "likelihoodLow.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizerLow = torch.optim.Adam(modelLow.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mllLow = gpytorch.mlls.ExactMarginalLogLikelihood(likelihoodLow, modelLow)\n",
    "\n",
    "training_iter_low = 10000\n",
    "for i in range(training_iter_low):\n",
    "    optimizerLow.zero_grad()\n",
    "    outputLow = modelLow(XLow)\n",
    "    lossLow = -mllLow(outputLow, YLow)\n",
    "    lossLow.backward()\n",
    "    if i %10==0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter_low, lossLow.item()))\n",
    "    optimizerLow.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21])\n",
      "[35.195095 35.881016 36.22503  35.195095 37.123    36.38499  35.150444\n",
      " 35.150444 35.623177 37.123    36.64236  35.5232   36.891277 37.503212\n",
      " 35.652092 35.150444 36.737743 36.98079  35.150444 35.195095 35.195095]\n",
      "tensor([17.6444, 17.8468, 18.2375, 17.6444, 18.3896, 18.0675, 17.5440, 17.5440,\n",
      "        17.3741, 18.3896, 18.1806, 17.8397, 18.2894, 18.5485, 17.7479, 17.5440,\n",
      "        18.5251, 18.6623, 17.5440, 17.6444, 17.6444])\n",
      "tensor([ 0.0938, -0.1875,  0.2500,  0.0938, -0.3438, -0.2500, -0.0625, -0.0625,\n",
      "        -0.8750, -0.3438, -0.2812,  0.1562, -0.3125, -0.4062, -0.1562, -0.0625,\n",
      "         0.3125,  0.3438, -0.0625,  0.0938,  0.0938])\n"
     ]
    }
   ],
   "source": [
    "modelLow.eval()\n",
    "likelihoodLow.eval()\n",
    "\n",
    "# The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)\n",
    "# See https://arxiv.org/abs/1803.06058\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihoodLow(modelLow(XHigh))\n",
    "    print(XHigh.shape)\n",
    "XHighNew = torch.stack([XHigh, observed_pred.mean]).T\n",
    "print(observed_pred.mean.numpy())\n",
    "XHighNew=XHighNew.mean(axis=1)\n",
    "print(XHighNew)\n",
    "print(XHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoodHigh = gpytorch.likelihoods.GaussianLikelihood()\n",
    "modelHigh = SpectralMixtureGPModel(XHighNew, YHigh, likelihoodHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10000 - Loss: 60.792\n",
      "Iter 11/10000 - Loss: 17.958\n",
      "Iter 21/10000 - Loss: 10.525\n",
      "Iter 31/10000 - Loss: 8.097\n",
      "Iter 41/10000 - Loss: 6.983\n",
      "Iter 51/10000 - Loss: 6.316\n",
      "Iter 61/10000 - Loss: 5.842\n",
      "Iter 71/10000 - Loss: 5.469\n",
      "Iter 81/10000 - Loss: 5.157\n",
      "Iter 91/10000 - Loss: 4.889\n",
      "Iter 101/10000 - Loss: 4.655\n",
      "Iter 111/10000 - Loss: 4.449\n",
      "Iter 121/10000 - Loss: 4.266\n",
      "Iter 131/10000 - Loss: 4.101\n",
      "Iter 141/10000 - Loss: 3.954\n",
      "Iter 151/10000 - Loss: 3.822\n",
      "Iter 161/10000 - Loss: 3.705\n",
      "Iter 171/10000 - Loss: 3.606\n",
      "Iter 181/10000 - Loss: 3.522\n",
      "Iter 191/10000 - Loss: 3.450\n",
      "Iter 201/10000 - Loss: 3.388\n",
      "Iter 211/10000 - Loss: 3.333\n",
      "Iter 221/10000 - Loss: 3.284\n",
      "Iter 231/10000 - Loss: 3.240\n",
      "Iter 241/10000 - Loss: 3.199\n",
      "Iter 251/10000 - Loss: 3.163\n",
      "Iter 261/10000 - Loss: 3.129\n",
      "Iter 271/10000 - Loss: 3.099\n",
      "Iter 281/10000 - Loss: 3.070\n",
      "Iter 291/10000 - Loss: 3.044\n",
      "Iter 301/10000 - Loss: 3.020\n",
      "Iter 311/10000 - Loss: 2.997\n",
      "Iter 321/10000 - Loss: 2.975\n",
      "Iter 331/10000 - Loss: 2.954\n",
      "Iter 341/10000 - Loss: 2.934\n",
      "Iter 351/10000 - Loss: 2.915\n",
      "Iter 361/10000 - Loss: 2.897\n",
      "Iter 371/10000 - Loss: 2.879\n",
      "Iter 381/10000 - Loss: 2.863\n",
      "Iter 391/10000 - Loss: 2.846\n",
      "Iter 401/10000 - Loss: 2.831\n",
      "Iter 411/10000 - Loss: 2.816\n",
      "Iter 421/10000 - Loss: 2.801\n",
      "Iter 431/10000 - Loss: 2.787\n",
      "Iter 441/10000 - Loss: 2.774\n",
      "Iter 451/10000 - Loss: 2.761\n",
      "Iter 461/10000 - Loss: 2.749\n",
      "Iter 471/10000 - Loss: 2.737\n",
      "Iter 481/10000 - Loss: 2.726\n",
      "Iter 491/10000 - Loss: 2.715\n",
      "Iter 501/10000 - Loss: 2.704\n",
      "Iter 511/10000 - Loss: 2.694\n",
      "Iter 521/10000 - Loss: 2.685\n",
      "Iter 531/10000 - Loss: 2.675\n",
      "Iter 541/10000 - Loss: 2.666\n",
      "Iter 551/10000 - Loss: 2.657\n",
      "Iter 561/10000 - Loss: 2.649\n",
      "Iter 571/10000 - Loss: 2.641\n",
      "Iter 581/10000 - Loss: 2.633\n",
      "Iter 591/10000 - Loss: 2.625\n",
      "Iter 601/10000 - Loss: 2.617\n",
      "Iter 611/10000 - Loss: 2.610\n",
      "Iter 621/10000 - Loss: 2.603\n",
      "Iter 631/10000 - Loss: 2.596\n",
      "Iter 641/10000 - Loss: 2.589\n",
      "Iter 651/10000 - Loss: 2.583\n",
      "Iter 661/10000 - Loss: 2.577\n",
      "Iter 671/10000 - Loss: 2.571\n",
      "Iter 681/10000 - Loss: 2.565\n",
      "Iter 691/10000 - Loss: 2.559\n",
      "Iter 701/10000 - Loss: 2.553\n",
      "Iter 711/10000 - Loss: 2.547\n",
      "Iter 721/10000 - Loss: 2.542\n",
      "Iter 731/10000 - Loss: 2.537\n",
      "Iter 741/10000 - Loss: 2.532\n",
      "Iter 751/10000 - Loss: 2.527\n",
      "Iter 761/10000 - Loss: 2.522\n",
      "Iter 771/10000 - Loss: 2.517\n",
      "Iter 781/10000 - Loss: 2.512\n",
      "Iter 791/10000 - Loss: 2.508\n",
      "Iter 801/10000 - Loss: 2.503\n",
      "Iter 811/10000 - Loss: 2.499\n",
      "Iter 821/10000 - Loss: 2.494\n",
      "Iter 831/10000 - Loss: 2.490\n",
      "Iter 841/10000 - Loss: 2.486\n",
      "Iter 851/10000 - Loss: 2.482\n",
      "Iter 861/10000 - Loss: 2.478\n",
      "Iter 871/10000 - Loss: 2.475\n",
      "Iter 881/10000 - Loss: 2.471\n",
      "Iter 891/10000 - Loss: 2.467\n",
      "Iter 901/10000 - Loss: 2.464\n",
      "Iter 911/10000 - Loss: 2.460\n",
      "Iter 921/10000 - Loss: 2.457\n",
      "Iter 931/10000 - Loss: 2.453\n",
      "Iter 941/10000 - Loss: 2.450\n",
      "Iter 951/10000 - Loss: 2.447\n",
      "Iter 961/10000 - Loss: 2.444\n",
      "Iter 971/10000 - Loss: 2.441\n",
      "Iter 981/10000 - Loss: 2.438\n",
      "Iter 991/10000 - Loss: 2.435\n",
      "Iter 1001/10000 - Loss: 2.432\n",
      "Iter 1011/10000 - Loss: 2.429\n",
      "Iter 1021/10000 - Loss: 2.426\n",
      "Iter 1031/10000 - Loss: 2.424\n",
      "Iter 1041/10000 - Loss: 2.421\n",
      "Iter 1051/10000 - Loss: 2.419\n",
      "Iter 1061/10000 - Loss: 2.416\n",
      "Iter 1071/10000 - Loss: 2.414\n",
      "Iter 1081/10000 - Loss: 2.411\n",
      "Iter 1091/10000 - Loss: 2.409\n",
      "Iter 1101/10000 - Loss: 2.406\n",
      "Iter 1111/10000 - Loss: 2.404\n",
      "Iter 1121/10000 - Loss: 2.402\n",
      "Iter 1131/10000 - Loss: 2.400\n",
      "Iter 1141/10000 - Loss: 2.398\n",
      "Iter 1151/10000 - Loss: 2.396\n",
      "Iter 1161/10000 - Loss: 2.394\n",
      "Iter 1171/10000 - Loss: 2.392\n",
      "Iter 1181/10000 - Loss: 2.390\n",
      "Iter 1191/10000 - Loss: 2.388\n",
      "Iter 1201/10000 - Loss: 2.386\n",
      "Iter 1211/10000 - Loss: 2.384\n",
      "Iter 1221/10000 - Loss: 2.383\n",
      "Iter 1231/10000 - Loss: 2.381\n",
      "Iter 1241/10000 - Loss: 2.379\n",
      "Iter 1251/10000 - Loss: 2.378\n",
      "Iter 1261/10000 - Loss: 2.376\n",
      "Iter 1271/10000 - Loss: 2.374\n",
      "Iter 1281/10000 - Loss: 2.373\n",
      "Iter 1291/10000 - Loss: 2.371\n",
      "Iter 1301/10000 - Loss: 2.370\n",
      "Iter 1311/10000 - Loss: 2.368\n",
      "Iter 1321/10000 - Loss: 2.367\n",
      "Iter 1331/10000 - Loss: 2.366\n",
      "Iter 1341/10000 - Loss: 2.364\n",
      "Iter 1351/10000 - Loss: 2.363\n",
      "Iter 1361/10000 - Loss: 2.362\n",
      "Iter 1371/10000 - Loss: 2.361\n",
      "Iter 1381/10000 - Loss: 2.359\n",
      "Iter 1391/10000 - Loss: 2.358\n",
      "Iter 1401/10000 - Loss: 2.357\n",
      "Iter 1411/10000 - Loss: 2.356\n",
      "Iter 1421/10000 - Loss: 2.355\n",
      "Iter 1431/10000 - Loss: 2.354\n",
      "Iter 1441/10000 - Loss: 2.353\n",
      "Iter 1451/10000 - Loss: 2.352\n",
      "Iter 1461/10000 - Loss: 2.351\n",
      "Iter 1471/10000 - Loss: 2.350\n",
      "Iter 1481/10000 - Loss: 2.349\n",
      "Iter 1491/10000 - Loss: 2.348\n",
      "Iter 1501/10000 - Loss: 2.347\n",
      "Iter 1511/10000 - Loss: 2.346\n",
      "Iter 1521/10000 - Loss: 2.345\n",
      "Iter 1531/10000 - Loss: 2.344\n",
      "Iter 1541/10000 - Loss: 2.344\n",
      "Iter 1551/10000 - Loss: 2.343\n",
      "Iter 1561/10000 - Loss: 2.342\n",
      "Iter 1571/10000 - Loss: 2.341\n",
      "Iter 1581/10000 - Loss: 2.341\n",
      "Iter 1591/10000 - Loss: 2.340\n",
      "Iter 1601/10000 - Loss: 2.339\n",
      "Iter 1611/10000 - Loss: 2.338\n",
      "Iter 1621/10000 - Loss: 2.338\n",
      "Iter 1631/10000 - Loss: 2.337\n",
      "Iter 1641/10000 - Loss: 2.337\n",
      "Iter 1651/10000 - Loss: 2.336\n",
      "Iter 1661/10000 - Loss: 2.335\n",
      "Iter 1671/10000 - Loss: 2.335\n",
      "Iter 1681/10000 - Loss: 2.334\n",
      "Iter 1691/10000 - Loss: 2.334\n",
      "Iter 1701/10000 - Loss: 2.333\n",
      "Iter 1711/10000 - Loss: 2.333\n",
      "Iter 1721/10000 - Loss: 2.332\n",
      "Iter 1731/10000 - Loss: 2.332\n",
      "Iter 1741/10000 - Loss: 2.331\n",
      "Iter 1751/10000 - Loss: 2.331\n",
      "Iter 1761/10000 - Loss: 2.330\n",
      "Iter 1771/10000 - Loss: 2.330\n",
      "Iter 1781/10000 - Loss: 2.329\n",
      "Iter 1791/10000 - Loss: 2.329\n",
      "Iter 1801/10000 - Loss: 2.328\n",
      "Iter 1811/10000 - Loss: 2.328\n",
      "Iter 1821/10000 - Loss: 2.328\n",
      "Iter 1831/10000 - Loss: 2.327\n",
      "Iter 1841/10000 - Loss: 2.327\n",
      "Iter 1851/10000 - Loss: 2.326\n",
      "Iter 1861/10000 - Loss: 2.326\n",
      "Iter 1871/10000 - Loss: 2.326\n",
      "Iter 1881/10000 - Loss: 2.325\n",
      "Iter 1891/10000 - Loss: 2.325\n",
      "Iter 1901/10000 - Loss: 2.325\n",
      "Iter 1911/10000 - Loss: 2.324\n",
      "Iter 1921/10000 - Loss: 2.324\n",
      "Iter 1931/10000 - Loss: 2.324\n",
      "Iter 1941/10000 - Loss: 2.324\n",
      "Iter 1951/10000 - Loss: 2.323\n",
      "Iter 1961/10000 - Loss: 2.323\n",
      "Iter 1971/10000 - Loss: 2.323\n",
      "Iter 1981/10000 - Loss: 2.322\n",
      "Iter 1991/10000 - Loss: 2.322\n",
      "Iter 2001/10000 - Loss: 2.322\n",
      "Iter 2011/10000 - Loss: 2.322\n",
      "Iter 2021/10000 - Loss: 2.321\n",
      "Iter 2031/10000 - Loss: 2.321\n",
      "Iter 2041/10000 - Loss: 2.321\n",
      "Iter 2051/10000 - Loss: 2.321\n",
      "Iter 2061/10000 - Loss: 2.320\n",
      "Iter 2071/10000 - Loss: 2.320\n",
      "Iter 2081/10000 - Loss: 2.320\n",
      "Iter 2091/10000 - Loss: 2.320\n",
      "Iter 2101/10000 - Loss: 2.319\n",
      "Iter 2111/10000 - Loss: 2.319\n",
      "Iter 2121/10000 - Loss: 2.319\n",
      "Iter 2131/10000 - Loss: 2.319\n",
      "Iter 2141/10000 - Loss: 2.318\n",
      "Iter 2151/10000 - Loss: 2.318\n",
      "Iter 2161/10000 - Loss: 2.318\n",
      "Iter 2171/10000 - Loss: 2.318\n",
      "Iter 2181/10000 - Loss: 2.318\n",
      "Iter 2191/10000 - Loss: 2.317\n",
      "Iter 2201/10000 - Loss: 2.317\n",
      "Iter 2211/10000 - Loss: 2.317\n",
      "Iter 2221/10000 - Loss: 2.317\n",
      "Iter 2231/10000 - Loss: 2.317\n",
      "Iter 2241/10000 - Loss: 2.316\n",
      "Iter 2251/10000 - Loss: 2.316\n",
      "Iter 2261/10000 - Loss: 2.316\n",
      "Iter 2271/10000 - Loss: 2.316\n",
      "Iter 2281/10000 - Loss: 2.316\n",
      "Iter 2291/10000 - Loss: 2.316\n",
      "Iter 2301/10000 - Loss: 2.315\n",
      "Iter 2311/10000 - Loss: 2.315\n",
      "Iter 2321/10000 - Loss: 2.315\n",
      "Iter 2331/10000 - Loss: 2.315\n",
      "Iter 2341/10000 - Loss: 2.315\n",
      "Iter 2351/10000 - Loss: 2.315\n",
      "Iter 2361/10000 - Loss: 2.314\n",
      "Iter 2371/10000 - Loss: 2.314\n",
      "Iter 2381/10000 - Loss: 2.314\n",
      "Iter 2391/10000 - Loss: 2.314\n",
      "Iter 2401/10000 - Loss: 2.314\n",
      "Iter 2411/10000 - Loss: 2.314\n",
      "Iter 2421/10000 - Loss: 2.314\n",
      "Iter 2431/10000 - Loss: 2.313\n",
      "Iter 2441/10000 - Loss: 2.313\n",
      "Iter 2451/10000 - Loss: 2.313\n",
      "Iter 2461/10000 - Loss: 2.313\n",
      "Iter 2471/10000 - Loss: 2.313\n",
      "Iter 2481/10000 - Loss: 2.313\n",
      "Iter 2491/10000 - Loss: 2.313\n",
      "Iter 2501/10000 - Loss: 2.313\n",
      "Iter 2511/10000 - Loss: 2.313\n",
      "Iter 2521/10000 - Loss: 2.312\n",
      "Iter 2531/10000 - Loss: 2.312\n",
      "Iter 2541/10000 - Loss: 2.312\n",
      "Iter 2551/10000 - Loss: 2.312\n",
      "Iter 2561/10000 - Loss: 2.312\n",
      "Iter 2571/10000 - Loss: 2.312\n",
      "Iter 2581/10000 - Loss: 2.312\n",
      "Iter 2591/10000 - Loss: 2.312\n",
      "Iter 2601/10000 - Loss: 2.312\n",
      "Iter 2611/10000 - Loss: 2.312\n",
      "Iter 2621/10000 - Loss: 2.312\n",
      "Iter 2631/10000 - Loss: 2.311\n",
      "Iter 2641/10000 - Loss: 2.311\n",
      "Iter 2651/10000 - Loss: 2.311\n",
      "Iter 2661/10000 - Loss: 2.311\n",
      "Iter 2671/10000 - Loss: 2.311\n",
      "Iter 2681/10000 - Loss: 2.311\n",
      "Iter 2691/10000 - Loss: 2.311\n",
      "Iter 2701/10000 - Loss: 2.311\n",
      "Iter 2711/10000 - Loss: 2.311\n",
      "Iter 2721/10000 - Loss: 2.311\n",
      "Iter 2731/10000 - Loss: 2.311\n",
      "Iter 2741/10000 - Loss: 2.311\n",
      "Iter 2751/10000 - Loss: 2.311\n",
      "Iter 2761/10000 - Loss: 2.311\n",
      "Iter 2771/10000 - Loss: 2.310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2781/10000 - Loss: 2.310\n",
      "Iter 2791/10000 - Loss: 2.310\n",
      "Iter 2801/10000 - Loss: 2.310\n",
      "Iter 2811/10000 - Loss: 2.310\n",
      "Iter 2821/10000 - Loss: 2.310\n",
      "Iter 2831/10000 - Loss: 2.310\n",
      "Iter 2841/10000 - Loss: 2.310\n",
      "Iter 2851/10000 - Loss: 2.310\n",
      "Iter 2861/10000 - Loss: 2.310\n",
      "Iter 2871/10000 - Loss: 2.310\n",
      "Iter 2881/10000 - Loss: 2.310\n",
      "Iter 2891/10000 - Loss: 2.310\n",
      "Iter 2901/10000 - Loss: 2.310\n",
      "Iter 2911/10000 - Loss: 2.310\n",
      "Iter 2921/10000 - Loss: 2.310\n",
      "Iter 2931/10000 - Loss: 2.309\n",
      "Iter 2941/10000 - Loss: 2.309\n",
      "Iter 2951/10000 - Loss: 2.309\n",
      "Iter 2961/10000 - Loss: 2.309\n",
      "Iter 2971/10000 - Loss: 2.309\n",
      "Iter 2981/10000 - Loss: 2.309\n",
      "Iter 2991/10000 - Loss: 2.309\n",
      "Iter 3001/10000 - Loss: 2.309\n",
      "Iter 3011/10000 - Loss: 2.309\n",
      "Iter 3021/10000 - Loss: 2.309\n",
      "Iter 3031/10000 - Loss: 2.309\n",
      "Iter 3041/10000 - Loss: 2.309\n",
      "Iter 3051/10000 - Loss: 2.309\n",
      "Iter 3061/10000 - Loss: 2.309\n",
      "Iter 3071/10000 - Loss: 2.309\n",
      "Iter 3081/10000 - Loss: 2.309\n",
      "Iter 3091/10000 - Loss: 2.309\n",
      "Iter 3101/10000 - Loss: 2.309\n",
      "Iter 3111/10000 - Loss: 2.308\n",
      "Iter 3121/10000 - Loss: 2.308\n",
      "Iter 3131/10000 - Loss: 2.308\n",
      "Iter 3141/10000 - Loss: 2.308\n",
      "Iter 3151/10000 - Loss: 2.308\n",
      "Iter 3161/10000 - Loss: 2.308\n",
      "Iter 3171/10000 - Loss: 2.308\n",
      "Iter 3181/10000 - Loss: 2.308\n",
      "Iter 3191/10000 - Loss: 2.308\n",
      "Iter 3201/10000 - Loss: 2.308\n",
      "Iter 3211/10000 - Loss: 2.308\n",
      "Iter 3221/10000 - Loss: 2.308\n",
      "Iter 3231/10000 - Loss: 2.308\n",
      "Iter 3241/10000 - Loss: 2.308\n",
      "Iter 3251/10000 - Loss: 2.308\n",
      "Iter 3261/10000 - Loss: 2.308\n",
      "Iter 3271/10000 - Loss: 2.307\n",
      "Iter 3281/10000 - Loss: 2.307\n",
      "Iter 3291/10000 - Loss: 2.307\n",
      "Iter 3301/10000 - Loss: 2.307\n",
      "Iter 3311/10000 - Loss: 2.307\n",
      "Iter 3321/10000 - Loss: 2.307\n",
      "Iter 3331/10000 - Loss: 2.307\n",
      "Iter 3341/10000 - Loss: 2.307\n",
      "Iter 3351/10000 - Loss: 2.307\n",
      "Iter 3361/10000 - Loss: 2.307\n",
      "Iter 3371/10000 - Loss: 2.307\n",
      "Iter 3381/10000 - Loss: 2.307\n",
      "Iter 3391/10000 - Loss: 2.307\n",
      "Iter 3401/10000 - Loss: 2.307\n",
      "Iter 3411/10000 - Loss: 2.307\n",
      "Iter 3421/10000 - Loss: 2.306\n",
      "Iter 3431/10000 - Loss: 2.306\n",
      "Iter 3441/10000 - Loss: 2.306\n",
      "Iter 3451/10000 - Loss: 2.306\n",
      "Iter 3461/10000 - Loss: 2.306\n",
      "Iter 3471/10000 - Loss: 2.306\n",
      "Iter 3481/10000 - Loss: 2.306\n",
      "Iter 3491/10000 - Loss: 2.306\n",
      "Iter 3501/10000 - Loss: 2.306\n",
      "Iter 3511/10000 - Loss: 2.306\n",
      "Iter 3521/10000 - Loss: 2.306\n",
      "Iter 3531/10000 - Loss: 2.306\n",
      "Iter 3541/10000 - Loss: 2.305\n",
      "Iter 3551/10000 - Loss: 2.305\n",
      "Iter 3561/10000 - Loss: 2.305\n",
      "Iter 3571/10000 - Loss: 2.305\n",
      "Iter 3581/10000 - Loss: 2.305\n",
      "Iter 3591/10000 - Loss: 2.305\n",
      "Iter 3601/10000 - Loss: 2.305\n",
      "Iter 3611/10000 - Loss: 2.305\n",
      "Iter 3621/10000 - Loss: 2.305\n",
      "Iter 3631/10000 - Loss: 2.305\n",
      "Iter 3641/10000 - Loss: 2.305\n",
      "Iter 3651/10000 - Loss: 2.304\n",
      "Iter 3661/10000 - Loss: 2.304\n",
      "Iter 3671/10000 - Loss: 2.304\n",
      "Iter 3681/10000 - Loss: 2.304\n",
      "Iter 3691/10000 - Loss: 2.304\n",
      "Iter 3701/10000 - Loss: 2.304\n",
      "Iter 3711/10000 - Loss: 2.304\n",
      "Iter 3721/10000 - Loss: 2.304\n",
      "Iter 3731/10000 - Loss: 2.304\n",
      "Iter 3741/10000 - Loss: 2.303\n",
      "Iter 3751/10000 - Loss: 2.303\n",
      "Iter 3761/10000 - Loss: 2.303\n",
      "Iter 3771/10000 - Loss: 2.303\n",
      "Iter 3781/10000 - Loss: 2.303\n",
      "Iter 3791/10000 - Loss: 2.303\n",
      "Iter 3801/10000 - Loss: 2.303\n",
      "Iter 3811/10000 - Loss: 2.303\n",
      "Iter 3821/10000 - Loss: 2.303\n",
      "Iter 3831/10000 - Loss: 2.302\n",
      "Iter 3841/10000 - Loss: 2.302\n",
      "Iter 3851/10000 - Loss: 2.302\n",
      "Iter 3861/10000 - Loss: 2.302\n",
      "Iter 3871/10000 - Loss: 2.302\n",
      "Iter 3881/10000 - Loss: 2.302\n",
      "Iter 3891/10000 - Loss: 2.302\n",
      "Iter 3901/10000 - Loss: 2.301\n",
      "Iter 3911/10000 - Loss: 2.301\n",
      "Iter 3921/10000 - Loss: 2.301\n",
      "Iter 3931/10000 - Loss: 2.301\n",
      "Iter 3941/10000 - Loss: 2.301\n",
      "Iter 3951/10000 - Loss: 2.301\n",
      "Iter 3961/10000 - Loss: 2.301\n",
      "Iter 3971/10000 - Loss: 2.300\n",
      "Iter 3981/10000 - Loss: 2.300\n",
      "Iter 3991/10000 - Loss: 2.300\n",
      "Iter 4001/10000 - Loss: 2.300\n",
      "Iter 4011/10000 - Loss: 2.300\n",
      "Iter 4021/10000 - Loss: 2.300\n",
      "Iter 4031/10000 - Loss: 2.299\n",
      "Iter 4041/10000 - Loss: 2.299\n",
      "Iter 4051/10000 - Loss: 2.299\n",
      "Iter 4061/10000 - Loss: 2.299\n",
      "Iter 4071/10000 - Loss: 2.299\n",
      "Iter 4081/10000 - Loss: 2.298\n",
      "Iter 4091/10000 - Loss: 2.298\n",
      "Iter 4101/10000 - Loss: 2.298\n",
      "Iter 4111/10000 - Loss: 2.298\n",
      "Iter 4121/10000 - Loss: 2.297\n",
      "Iter 4131/10000 - Loss: 2.297\n",
      "Iter 4141/10000 - Loss: 2.297\n",
      "Iter 4151/10000 - Loss: 2.297\n",
      "Iter 4161/10000 - Loss: 2.296\n",
      "Iter 4171/10000 - Loss: 2.296\n",
      "Iter 4181/10000 - Loss: 2.296\n",
      "Iter 4191/10000 - Loss: 2.296\n",
      "Iter 4201/10000 - Loss: 2.295\n",
      "Iter 4211/10000 - Loss: 2.295\n",
      "Iter 4221/10000 - Loss: 2.295\n",
      "Iter 4231/10000 - Loss: 2.294\n",
      "Iter 4241/10000 - Loss: 2.294\n",
      "Iter 4251/10000 - Loss: 2.294\n",
      "Iter 4261/10000 - Loss: 2.293\n",
      "Iter 4271/10000 - Loss: 2.293\n",
      "Iter 4281/10000 - Loss: 2.293\n",
      "Iter 4291/10000 - Loss: 2.292\n",
      "Iter 4301/10000 - Loss: 2.292\n",
      "Iter 4311/10000 - Loss: 2.292\n",
      "Iter 4321/10000 - Loss: 2.291\n",
      "Iter 4331/10000 - Loss: 2.291\n",
      "Iter 4341/10000 - Loss: 2.290\n",
      "Iter 4351/10000 - Loss: 2.290\n",
      "Iter 4361/10000 - Loss: 2.289\n",
      "Iter 4371/10000 - Loss: 2.289\n",
      "Iter 4381/10000 - Loss: 2.289\n",
      "Iter 4391/10000 - Loss: 2.288\n",
      "Iter 4401/10000 - Loss: 2.288\n",
      "Iter 4411/10000 - Loss: 2.287\n",
      "Iter 4421/10000 - Loss: 2.287\n",
      "Iter 4431/10000 - Loss: 2.286\n",
      "Iter 4441/10000 - Loss: 2.286\n",
      "Iter 4451/10000 - Loss: 2.285\n",
      "Iter 4461/10000 - Loss: 2.285\n",
      "Iter 4471/10000 - Loss: 2.284\n",
      "Iter 4481/10000 - Loss: 2.283\n",
      "Iter 4491/10000 - Loss: 2.283\n",
      "Iter 4501/10000 - Loss: 2.282\n",
      "Iter 4511/10000 - Loss: 2.282\n",
      "Iter 4521/10000 - Loss: 2.281\n",
      "Iter 4531/10000 - Loss: 2.280\n",
      "Iter 4541/10000 - Loss: 2.280\n",
      "Iter 4551/10000 - Loss: 2.279\n",
      "Iter 4561/10000 - Loss: 2.279\n",
      "Iter 4571/10000 - Loss: 2.278\n",
      "Iter 4581/10000 - Loss: 2.277\n",
      "Iter 4591/10000 - Loss: 2.277\n",
      "Iter 4601/10000 - Loss: 2.276\n",
      "Iter 4611/10000 - Loss: 2.275\n",
      "Iter 4621/10000 - Loss: 2.274\n",
      "Iter 4631/10000 - Loss: 2.274\n",
      "Iter 4641/10000 - Loss: 2.273\n",
      "Iter 4651/10000 - Loss: 2.272\n",
      "Iter 4661/10000 - Loss: 2.272\n",
      "Iter 4671/10000 - Loss: 2.271\n",
      "Iter 4681/10000 - Loss: 2.270\n",
      "Iter 4691/10000 - Loss: 2.270\n",
      "Iter 4701/10000 - Loss: 2.269\n",
      "Iter 4711/10000 - Loss: 2.268\n",
      "Iter 4721/10000 - Loss: 2.268\n",
      "Iter 4731/10000 - Loss: 2.267\n",
      "Iter 4741/10000 - Loss: 2.267\n",
      "Iter 4751/10000 - Loss: 2.266\n",
      "Iter 4761/10000 - Loss: 2.265\n",
      "Iter 4771/10000 - Loss: 2.265\n",
      "Iter 4781/10000 - Loss: 2.264\n",
      "Iter 4791/10000 - Loss: 2.263\n",
      "Iter 4801/10000 - Loss: 2.262\n",
      "Iter 4811/10000 - Loss: 2.261\n",
      "Iter 4821/10000 - Loss: 2.260\n",
      "Iter 4831/10000 - Loss: 2.258\n",
      "Iter 4841/10000 - Loss: 2.255\n",
      "Iter 4851/10000 - Loss: 2.247\n",
      "Iter 4861/10000 - Loss: 2.222\n",
      "Iter 4871/10000 - Loss: 2.200\n",
      "Iter 4881/10000 - Loss: 2.193\n",
      "Iter 4891/10000 - Loss: 2.190\n",
      "Iter 4901/10000 - Loss: 2.188\n",
      "Iter 4911/10000 - Loss: 2.187\n",
      "Iter 4921/10000 - Loss: 2.186\n",
      "Iter 4931/10000 - Loss: 2.185\n",
      "Iter 4941/10000 - Loss: 2.184\n",
      "Iter 4951/10000 - Loss: 2.183\n",
      "Iter 4961/10000 - Loss: 2.181\n",
      "Iter 4971/10000 - Loss: 2.180\n",
      "Iter 4981/10000 - Loss: 2.179\n",
      "Iter 4991/10000 - Loss: 2.178\n",
      "Iter 5001/10000 - Loss: 2.177\n",
      "Iter 5011/10000 - Loss: 2.176\n",
      "Iter 5021/10000 - Loss: 2.175\n",
      "Iter 5031/10000 - Loss: 2.174\n",
      "Iter 5041/10000 - Loss: 2.173\n",
      "Iter 5051/10000 - Loss: 2.171\n",
      "Iter 5061/10000 - Loss: 2.170\n",
      "Iter 5071/10000 - Loss: 2.169\n",
      "Iter 5081/10000 - Loss: 2.168\n",
      "Iter 5091/10000 - Loss: 2.166\n",
      "Iter 5101/10000 - Loss: 2.165\n",
      "Iter 5111/10000 - Loss: 2.164\n",
      "Iter 5121/10000 - Loss: 2.163\n",
      "Iter 5131/10000 - Loss: 2.162\n",
      "Iter 5141/10000 - Loss: 2.160\n",
      "Iter 5151/10000 - Loss: 2.159\n",
      "Iter 5161/10000 - Loss: 2.158\n",
      "Iter 5171/10000 - Loss: 2.157\n",
      "Iter 5181/10000 - Loss: 2.156\n",
      "Iter 5191/10000 - Loss: 2.155\n",
      "Iter 5201/10000 - Loss: 2.154\n",
      "Iter 5211/10000 - Loss: 2.153\n",
      "Iter 5221/10000 - Loss: 2.152\n",
      "Iter 5231/10000 - Loss: 2.151\n",
      "Iter 5241/10000 - Loss: 2.150\n",
      "Iter 5251/10000 - Loss: 2.150\n",
      "Iter 5261/10000 - Loss: 2.149\n",
      "Iter 5271/10000 - Loss: 2.148\n",
      "Iter 5281/10000 - Loss: 2.148\n",
      "Iter 5291/10000 - Loss: 2.147\n",
      "Iter 5301/10000 - Loss: 2.146\n",
      "Iter 5311/10000 - Loss: 2.146\n",
      "Iter 5321/10000 - Loss: 2.145\n",
      "Iter 5331/10000 - Loss: 2.145\n",
      "Iter 5341/10000 - Loss: 2.144\n",
      "Iter 5351/10000 - Loss: 2.144\n",
      "Iter 5361/10000 - Loss: 2.144\n",
      "Iter 5371/10000 - Loss: 2.143\n",
      "Iter 5381/10000 - Loss: 2.143\n",
      "Iter 5391/10000 - Loss: 2.142\n",
      "Iter 5401/10000 - Loss: 2.142\n",
      "Iter 5411/10000 - Loss: 2.142\n",
      "Iter 5421/10000 - Loss: 2.142\n",
      "Iter 5431/10000 - Loss: 2.141\n",
      "Iter 5441/10000 - Loss: 2.141\n",
      "Iter 5451/10000 - Loss: 2.141\n",
      "Iter 5461/10000 - Loss: 2.141\n",
      "Iter 5471/10000 - Loss: 2.140\n",
      "Iter 5481/10000 - Loss: 2.140\n",
      "Iter 5491/10000 - Loss: 2.140\n",
      "Iter 5501/10000 - Loss: 2.140\n",
      "Iter 5511/10000 - Loss: 2.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5521/10000 - Loss: 2.139\n",
      "Iter 5531/10000 - Loss: 2.139\n",
      "Iter 5541/10000 - Loss: 2.139\n",
      "Iter 5551/10000 - Loss: 2.139\n",
      "Iter 5561/10000 - Loss: 2.139\n",
      "Iter 5571/10000 - Loss: 2.139\n",
      "Iter 5581/10000 - Loss: 2.138\n",
      "Iter 5591/10000 - Loss: 2.138\n",
      "Iter 5601/10000 - Loss: 2.138\n",
      "Iter 5611/10000 - Loss: 2.138\n",
      "Iter 5621/10000 - Loss: 2.138\n",
      "Iter 5631/10000 - Loss: 2.138\n",
      "Iter 5641/10000 - Loss: 2.138\n",
      "Iter 5651/10000 - Loss: 2.138\n",
      "Iter 5661/10000 - Loss: 2.138\n",
      "Iter 5671/10000 - Loss: 2.138\n",
      "Iter 5681/10000 - Loss: 2.137\n",
      "Iter 5691/10000 - Loss: 2.137\n",
      "Iter 5701/10000 - Loss: 2.137\n",
      "Iter 5711/10000 - Loss: 2.137\n",
      "Iter 5721/10000 - Loss: 2.137\n",
      "Iter 5731/10000 - Loss: 2.137\n",
      "Iter 5741/10000 - Loss: 2.137\n",
      "Iter 5751/10000 - Loss: 2.137\n",
      "Iter 5761/10000 - Loss: 2.137\n",
      "Iter 5771/10000 - Loss: 2.137\n",
      "Iter 5781/10000 - Loss: 2.137\n",
      "Iter 5791/10000 - Loss: 2.137\n",
      "Iter 5801/10000 - Loss: 2.137\n",
      "Iter 5811/10000 - Loss: 2.137\n",
      "Iter 5821/10000 - Loss: 2.136\n",
      "Iter 5831/10000 - Loss: 2.136\n",
      "Iter 5841/10000 - Loss: 2.136\n",
      "Iter 5851/10000 - Loss: 2.136\n",
      "Iter 5861/10000 - Loss: 2.136\n",
      "Iter 5871/10000 - Loss: 2.136\n",
      "Iter 5881/10000 - Loss: 2.136\n",
      "Iter 5891/10000 - Loss: 2.136\n",
      "Iter 5901/10000 - Loss: 2.136\n",
      "Iter 5911/10000 - Loss: 2.136\n",
      "Iter 5921/10000 - Loss: 2.136\n",
      "Iter 5931/10000 - Loss: 2.136\n",
      "Iter 5941/10000 - Loss: 2.136\n",
      "Iter 5951/10000 - Loss: 2.136\n",
      "Iter 5961/10000 - Loss: 2.136\n",
      "Iter 5971/10000 - Loss: 2.136\n",
      "Iter 5981/10000 - Loss: 2.136\n",
      "Iter 5991/10000 - Loss: 2.136\n",
      "Iter 6001/10000 - Loss: 2.136\n",
      "Iter 6011/10000 - Loss: 2.136\n",
      "Iter 6021/10000 - Loss: 2.136\n",
      "Iter 6031/10000 - Loss: 2.136\n",
      "Iter 6041/10000 - Loss: 2.136\n",
      "Iter 6051/10000 - Loss: 2.136\n",
      "Iter 6061/10000 - Loss: 2.136\n",
      "Iter 6071/10000 - Loss: 2.136\n",
      "Iter 6081/10000 - Loss: 2.136\n",
      "Iter 6091/10000 - Loss: 2.136\n",
      "Iter 6101/10000 - Loss: 2.135\n",
      "Iter 6111/10000 - Loss: 2.135\n",
      "Iter 6121/10000 - Loss: 2.135\n",
      "Iter 6131/10000 - Loss: 2.135\n",
      "Iter 6141/10000 - Loss: 2.135\n",
      "Iter 6151/10000 - Loss: 2.135\n",
      "Iter 6161/10000 - Loss: 2.135\n",
      "Iter 6171/10000 - Loss: 2.135\n",
      "Iter 6181/10000 - Loss: 2.135\n",
      "Iter 6191/10000 - Loss: 2.135\n",
      "Iter 6201/10000 - Loss: 2.135\n",
      "Iter 6211/10000 - Loss: 2.135\n",
      "Iter 6221/10000 - Loss: 2.135\n",
      "Iter 6231/10000 - Loss: 2.135\n",
      "Iter 6241/10000 - Loss: 2.135\n",
      "Iter 6251/10000 - Loss: 2.135\n",
      "Iter 6261/10000 - Loss: 2.135\n",
      "Iter 6271/10000 - Loss: 2.135\n",
      "Iter 6281/10000 - Loss: 2.135\n",
      "Iter 6291/10000 - Loss: 2.135\n",
      "Iter 6301/10000 - Loss: 2.135\n",
      "Iter 6311/10000 - Loss: 2.135\n",
      "Iter 6321/10000 - Loss: 2.135\n",
      "Iter 6331/10000 - Loss: 2.135\n",
      "Iter 6341/10000 - Loss: 2.135\n",
      "Iter 6351/10000 - Loss: 2.135\n",
      "Iter 6361/10000 - Loss: 2.135\n",
      "Iter 6371/10000 - Loss: 2.135\n",
      "Iter 6381/10000 - Loss: 2.135\n",
      "Iter 6391/10000 - Loss: 2.135\n",
      "Iter 6401/10000 - Loss: 2.135\n",
      "Iter 6411/10000 - Loss: 2.135\n",
      "Iter 6421/10000 - Loss: 2.135\n",
      "Iter 6431/10000 - Loss: 2.135\n",
      "Iter 6441/10000 - Loss: 2.135\n",
      "Iter 6451/10000 - Loss: 2.135\n",
      "Iter 6461/10000 - Loss: 2.135\n",
      "Iter 6471/10000 - Loss: 2.135\n",
      "Iter 6481/10000 - Loss: 2.135\n",
      "Iter 6491/10000 - Loss: 2.135\n",
      "Iter 6501/10000 - Loss: 2.135\n",
      "Iter 6511/10000 - Loss: 2.135\n",
      "Iter 6521/10000 - Loss: 2.135\n",
      "Iter 6531/10000 - Loss: 2.135\n",
      "Iter 6541/10000 - Loss: 2.135\n",
      "Iter 6551/10000 - Loss: 2.135\n",
      "Iter 6561/10000 - Loss: 2.135\n",
      "Iter 6571/10000 - Loss: 2.135\n",
      "Iter 6581/10000 - Loss: 2.135\n",
      "Iter 6591/10000 - Loss: 2.135\n",
      "Iter 6601/10000 - Loss: 2.135\n",
      "Iter 6611/10000 - Loss: 2.135\n",
      "Iter 6621/10000 - Loss: 2.135\n",
      "Iter 6631/10000 - Loss: 2.135\n",
      "Iter 6641/10000 - Loss: 2.135\n",
      "Iter 6651/10000 - Loss: 2.135\n",
      "Iter 6661/10000 - Loss: 2.135\n",
      "Iter 6671/10000 - Loss: 2.135\n",
      "Iter 6681/10000 - Loss: 2.135\n",
      "Iter 6691/10000 - Loss: 2.135\n",
      "Iter 6701/10000 - Loss: 2.135\n",
      "Iter 6711/10000 - Loss: 2.135\n",
      "Iter 6721/10000 - Loss: 2.135\n",
      "Iter 6731/10000 - Loss: 2.135\n",
      "Iter 6741/10000 - Loss: 2.135\n",
      "Iter 6751/10000 - Loss: 2.135\n",
      "Iter 6761/10000 - Loss: 2.135\n",
      "Iter 6771/10000 - Loss: 2.135\n",
      "Iter 6781/10000 - Loss: 2.135\n",
      "Iter 6791/10000 - Loss: 2.135\n",
      "Iter 6801/10000 - Loss: 2.135\n",
      "Iter 6811/10000 - Loss: 2.135\n",
      "Iter 6821/10000 - Loss: 2.135\n",
      "Iter 6831/10000 - Loss: 2.135\n",
      "Iter 6841/10000 - Loss: 2.135\n",
      "Iter 6851/10000 - Loss: 2.135\n",
      "Iter 6861/10000 - Loss: 2.134\n",
      "Iter 6871/10000 - Loss: 2.134\n",
      "Iter 6881/10000 - Loss: 2.134\n",
      "Iter 6891/10000 - Loss: 2.134\n",
      "Iter 6901/10000 - Loss: 2.134\n",
      "Iter 6911/10000 - Loss: 2.134\n",
      "Iter 6921/10000 - Loss: 2.134\n",
      "Iter 6931/10000 - Loss: 2.134\n",
      "Iter 6941/10000 - Loss: 2.134\n",
      "Iter 6951/10000 - Loss: 2.134\n",
      "Iter 6961/10000 - Loss: 2.134\n",
      "Iter 6971/10000 - Loss: 2.134\n",
      "Iter 6981/10000 - Loss: 2.134\n",
      "Iter 6991/10000 - Loss: 2.134\n",
      "Iter 7001/10000 - Loss: 2.134\n",
      "Iter 7011/10000 - Loss: 2.134\n",
      "Iter 7021/10000 - Loss: 2.134\n",
      "Iter 7031/10000 - Loss: 2.134\n",
      "Iter 7041/10000 - Loss: 2.134\n",
      "Iter 7051/10000 - Loss: 2.134\n",
      "Iter 7061/10000 - Loss: 2.134\n",
      "Iter 7071/10000 - Loss: 2.134\n",
      "Iter 7081/10000 - Loss: 2.135\n",
      "Iter 7091/10000 - Loss: 2.134\n",
      "Iter 7101/10000 - Loss: 2.134\n",
      "Iter 7111/10000 - Loss: 2.134\n",
      "Iter 7121/10000 - Loss: 2.134\n",
      "Iter 7131/10000 - Loss: 2.134\n",
      "Iter 7141/10000 - Loss: 2.134\n",
      "Iter 7151/10000 - Loss: 2.134\n",
      "Iter 7161/10000 - Loss: 2.134\n",
      "Iter 7171/10000 - Loss: 2.134\n",
      "Iter 7181/10000 - Loss: 2.134\n",
      "Iter 7191/10000 - Loss: 2.134\n",
      "Iter 7201/10000 - Loss: 2.134\n",
      "Iter 7211/10000 - Loss: 2.134\n",
      "Iter 7221/10000 - Loss: 2.134\n",
      "Iter 7231/10000 - Loss: 2.134\n",
      "Iter 7241/10000 - Loss: 2.134\n",
      "Iter 7251/10000 - Loss: 2.134\n",
      "Iter 7261/10000 - Loss: 2.134\n",
      "Iter 7271/10000 - Loss: 2.134\n",
      "Iter 7281/10000 - Loss: 2.134\n",
      "Iter 7291/10000 - Loss: 2.134\n",
      "Iter 7301/10000 - Loss: 2.134\n",
      "Iter 7311/10000 - Loss: 2.134\n",
      "Iter 7321/10000 - Loss: 2.134\n",
      "Iter 7331/10000 - Loss: 2.134\n",
      "Iter 7341/10000 - Loss: 2.134\n",
      "Iter 7351/10000 - Loss: 2.134\n",
      "Iter 7361/10000 - Loss: 2.134\n",
      "Iter 7371/10000 - Loss: 2.134\n",
      "Iter 7381/10000 - Loss: 2.134\n",
      "Iter 7391/10000 - Loss: 2.134\n",
      "Iter 7401/10000 - Loss: 2.134\n",
      "Iter 7411/10000 - Loss: 2.134\n",
      "Iter 7421/10000 - Loss: 2.134\n",
      "Iter 7431/10000 - Loss: 2.134\n",
      "Iter 7441/10000 - Loss: 2.134\n",
      "Iter 7451/10000 - Loss: 2.134\n",
      "Iter 7461/10000 - Loss: 2.134\n",
      "Iter 7471/10000 - Loss: 2.134\n",
      "Iter 7481/10000 - Loss: 2.134\n",
      "Iter 7491/10000 - Loss: 2.134\n",
      "Iter 7501/10000 - Loss: 2.134\n",
      "Iter 7511/10000 - Loss: 2.134\n",
      "Iter 7521/10000 - Loss: 2.134\n",
      "Iter 7531/10000 - Loss: 2.134\n",
      "Iter 7541/10000 - Loss: 2.134\n",
      "Iter 7551/10000 - Loss: 2.134\n",
      "Iter 7561/10000 - Loss: 2.134\n",
      "Iter 7571/10000 - Loss: 2.134\n",
      "Iter 7581/10000 - Loss: 2.134\n",
      "Iter 7591/10000 - Loss: 2.134\n",
      "Iter 7601/10000 - Loss: 2.134\n",
      "Iter 7611/10000 - Loss: 2.134\n",
      "Iter 7621/10000 - Loss: 2.134\n",
      "Iter 7631/10000 - Loss: 2.134\n",
      "Iter 7641/10000 - Loss: 2.134\n",
      "Iter 7651/10000 - Loss: 2.134\n",
      "Iter 7661/10000 - Loss: 2.134\n",
      "Iter 7671/10000 - Loss: 2.134\n",
      "Iter 7681/10000 - Loss: 2.134\n",
      "Iter 7691/10000 - Loss: 2.134\n",
      "Iter 7701/10000 - Loss: 2.134\n",
      "Iter 7711/10000 - Loss: 2.134\n",
      "Iter 7721/10000 - Loss: 2.134\n",
      "Iter 7731/10000 - Loss: 2.134\n",
      "Iter 7741/10000 - Loss: 2.134\n",
      "Iter 7751/10000 - Loss: 2.134\n",
      "Iter 7761/10000 - Loss: 2.134\n",
      "Iter 7771/10000 - Loss: 2.134\n",
      "Iter 7781/10000 - Loss: 2.134\n",
      "Iter 7791/10000 - Loss: 2.134\n",
      "Iter 7801/10000 - Loss: 2.134\n",
      "Iter 7811/10000 - Loss: 2.134\n",
      "Iter 7821/10000 - Loss: 2.134\n",
      "Iter 7831/10000 - Loss: 2.134\n",
      "Iter 7841/10000 - Loss: 2.134\n",
      "Iter 7851/10000 - Loss: 2.134\n",
      "Iter 7861/10000 - Loss: 2.134\n",
      "Iter 7871/10000 - Loss: 2.134\n",
      "Iter 7881/10000 - Loss: 2.134\n",
      "Iter 7891/10000 - Loss: 2.134\n",
      "Iter 7901/10000 - Loss: 2.134\n",
      "Iter 7911/10000 - Loss: 2.134\n",
      "Iter 7921/10000 - Loss: 2.134\n",
      "Iter 7931/10000 - Loss: 2.134\n",
      "Iter 7941/10000 - Loss: 2.134\n",
      "Iter 7951/10000 - Loss: 2.134\n",
      "Iter 7961/10000 - Loss: 2.134\n",
      "Iter 7971/10000 - Loss: 2.134\n",
      "Iter 7981/10000 - Loss: 2.134\n",
      "Iter 7991/10000 - Loss: 2.134\n",
      "Iter 8001/10000 - Loss: 2.134\n",
      "Iter 8011/10000 - Loss: 2.134\n",
      "Iter 8021/10000 - Loss: 2.134\n",
      "Iter 8031/10000 - Loss: 2.134\n",
      "Iter 8041/10000 - Loss: 2.134\n",
      "Iter 8051/10000 - Loss: 2.134\n",
      "Iter 8061/10000 - Loss: 2.134\n",
      "Iter 8071/10000 - Loss: 2.134\n",
      "Iter 8081/10000 - Loss: 2.134\n",
      "Iter 8091/10000 - Loss: 2.134\n",
      "Iter 8101/10000 - Loss: 2.134\n",
      "Iter 8111/10000 - Loss: 2.134\n",
      "Iter 8121/10000 - Loss: 2.134\n",
      "Iter 8131/10000 - Loss: 2.134\n",
      "Iter 8141/10000 - Loss: 2.134\n",
      "Iter 8151/10000 - Loss: 2.134\n",
      "Iter 8161/10000 - Loss: 2.134\n",
      "Iter 8171/10000 - Loss: 2.134\n",
      "Iter 8181/10000 - Loss: 2.134\n",
      "Iter 8191/10000 - Loss: 2.134\n",
      "Iter 8201/10000 - Loss: 2.134\n",
      "Iter 8211/10000 - Loss: 2.134\n",
      "Iter 8221/10000 - Loss: 2.134\n",
      "Iter 8231/10000 - Loss: 2.134\n",
      "Iter 8241/10000 - Loss: 2.134\n",
      "Iter 8251/10000 - Loss: 2.134\n",
      "Iter 8261/10000 - Loss: 2.134\n",
      "Iter 8271/10000 - Loss: 2.134\n",
      "Iter 8281/10000 - Loss: 2.134\n",
      "Iter 8291/10000 - Loss: 2.134\n",
      "Iter 8301/10000 - Loss: 2.134\n",
      "Iter 8311/10000 - Loss: 2.134\n",
      "Iter 8321/10000 - Loss: 2.134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8331/10000 - Loss: 2.134\n",
      "Iter 8341/10000 - Loss: 2.134\n",
      "Iter 8351/10000 - Loss: 2.134\n",
      "Iter 8361/10000 - Loss: 2.134\n",
      "Iter 8371/10000 - Loss: 2.134\n",
      "Iter 8381/10000 - Loss: 2.134\n",
      "Iter 8391/10000 - Loss: 2.134\n",
      "Iter 8401/10000 - Loss: 2.134\n",
      "Iter 8411/10000 - Loss: 2.134\n",
      "Iter 8421/10000 - Loss: 2.134\n",
      "Iter 8431/10000 - Loss: 2.134\n",
      "Iter 8441/10000 - Loss: 2.134\n",
      "Iter 8451/10000 - Loss: 2.134\n",
      "Iter 8461/10000 - Loss: 2.134\n",
      "Iter 8471/10000 - Loss: 2.134\n",
      "Iter 8481/10000 - Loss: 2.134\n",
      "Iter 8491/10000 - Loss: 2.134\n",
      "Iter 8501/10000 - Loss: 2.134\n",
      "Iter 8511/10000 - Loss: 2.134\n",
      "Iter 8521/10000 - Loss: 2.134\n",
      "Iter 8531/10000 - Loss: 2.134\n",
      "Iter 8541/10000 - Loss: 2.134\n",
      "Iter 8551/10000 - Loss: 2.134\n",
      "Iter 8561/10000 - Loss: 2.134\n",
      "Iter 8571/10000 - Loss: 2.134\n",
      "Iter 8581/10000 - Loss: 2.134\n",
      "Iter 8591/10000 - Loss: 2.134\n",
      "Iter 8601/10000 - Loss: 2.134\n",
      "Iter 8611/10000 - Loss: 2.134\n",
      "Iter 8621/10000 - Loss: 2.134\n",
      "Iter 8631/10000 - Loss: 2.134\n",
      "Iter 8641/10000 - Loss: 2.134\n",
      "Iter 8651/10000 - Loss: 2.134\n",
      "Iter 8661/10000 - Loss: 2.134\n",
      "Iter 8671/10000 - Loss: 2.134\n",
      "Iter 8681/10000 - Loss: 2.134\n",
      "Iter 8691/10000 - Loss: 2.134\n",
      "Iter 8701/10000 - Loss: 2.134\n",
      "Iter 8711/10000 - Loss: 2.134\n",
      "Iter 8721/10000 - Loss: 2.134\n",
      "Iter 8731/10000 - Loss: 2.134\n",
      "Iter 8741/10000 - Loss: 2.134\n",
      "Iter 8751/10000 - Loss: 2.134\n",
      "Iter 8761/10000 - Loss: 2.134\n",
      "Iter 8771/10000 - Loss: 2.134\n",
      "Iter 8781/10000 - Loss: 2.134\n",
      "Iter 8791/10000 - Loss: 2.134\n",
      "Iter 8801/10000 - Loss: 2.134\n",
      "Iter 8811/10000 - Loss: 2.134\n",
      "Iter 8821/10000 - Loss: 2.134\n",
      "Iter 8831/10000 - Loss: 2.134\n",
      "Iter 8841/10000 - Loss: 2.134\n",
      "Iter 8851/10000 - Loss: 2.134\n",
      "Iter 8861/10000 - Loss: 2.134\n",
      "Iter 8871/10000 - Loss: 2.134\n",
      "Iter 8881/10000 - Loss: 2.134\n",
      "Iter 8891/10000 - Loss: 2.134\n",
      "Iter 8901/10000 - Loss: 2.134\n",
      "Iter 8911/10000 - Loss: 2.134\n",
      "Iter 8921/10000 - Loss: 2.134\n",
      "Iter 8931/10000 - Loss: 2.134\n",
      "Iter 8941/10000 - Loss: 2.134\n",
      "Iter 8951/10000 - Loss: 2.134\n",
      "Iter 8961/10000 - Loss: 2.134\n",
      "Iter 8971/10000 - Loss: 2.134\n",
      "Iter 8981/10000 - Loss: 2.134\n",
      "Iter 8991/10000 - Loss: 2.134\n",
      "Iter 9001/10000 - Loss: 2.134\n",
      "Iter 9011/10000 - Loss: 2.134\n",
      "Iter 9021/10000 - Loss: 2.134\n",
      "Iter 9031/10000 - Loss: 2.134\n",
      "Iter 9041/10000 - Loss: 2.134\n",
      "Iter 9051/10000 - Loss: 2.134\n",
      "Iter 9061/10000 - Loss: 2.134\n",
      "Iter 9071/10000 - Loss: 2.134\n",
      "Iter 9081/10000 - Loss: 2.134\n",
      "Iter 9091/10000 - Loss: 2.134\n",
      "Iter 9101/10000 - Loss: 2.134\n",
      "Iter 9111/10000 - Loss: 2.134\n",
      "Iter 9121/10000 - Loss: 2.134\n",
      "Iter 9131/10000 - Loss: 2.134\n",
      "Iter 9141/10000 - Loss: 2.134\n",
      "Iter 9151/10000 - Loss: 2.134\n",
      "Iter 9161/10000 - Loss: 2.134\n",
      "Iter 9171/10000 - Loss: 2.134\n",
      "Iter 9181/10000 - Loss: 2.134\n",
      "Iter 9191/10000 - Loss: 2.134\n",
      "Iter 9201/10000 - Loss: 2.134\n",
      "Iter 9211/10000 - Loss: 2.134\n",
      "Iter 9221/10000 - Loss: 2.134\n",
      "Iter 9231/10000 - Loss: 2.134\n",
      "Iter 9241/10000 - Loss: 2.134\n",
      "Iter 9251/10000 - Loss: 2.134\n",
      "Iter 9261/10000 - Loss: 2.134\n",
      "Iter 9271/10000 - Loss: 2.134\n",
      "Iter 9281/10000 - Loss: 2.134\n",
      "Iter 9291/10000 - Loss: 2.134\n",
      "Iter 9301/10000 - Loss: 2.134\n",
      "Iter 9311/10000 - Loss: 2.134\n",
      "Iter 9321/10000 - Loss: 2.134\n",
      "Iter 9331/10000 - Loss: 2.134\n",
      "Iter 9341/10000 - Loss: 2.134\n",
      "Iter 9351/10000 - Loss: 2.134\n",
      "Iter 9361/10000 - Loss: 2.134\n",
      "Iter 9371/10000 - Loss: 2.134\n",
      "Iter 9381/10000 - Loss: 2.134\n",
      "Iter 9391/10000 - Loss: 2.134\n",
      "Iter 9401/10000 - Loss: 2.134\n",
      "Iter 9411/10000 - Loss: 2.134\n",
      "Iter 9421/10000 - Loss: 2.134\n",
      "Iter 9431/10000 - Loss: 2.134\n",
      "Iter 9441/10000 - Loss: 2.134\n",
      "Iter 9451/10000 - Loss: 2.134\n",
      "Iter 9461/10000 - Loss: 2.134\n",
      "Iter 9471/10000 - Loss: 2.134\n",
      "Iter 9481/10000 - Loss: 2.134\n",
      "Iter 9491/10000 - Loss: 2.134\n",
      "Iter 9501/10000 - Loss: 2.134\n",
      "Iter 9511/10000 - Loss: 2.134\n",
      "Iter 9521/10000 - Loss: 2.134\n",
      "Iter 9531/10000 - Loss: 2.134\n",
      "Iter 9541/10000 - Loss: 2.134\n",
      "Iter 9551/10000 - Loss: 2.134\n",
      "Iter 9561/10000 - Loss: 2.134\n",
      "Iter 9571/10000 - Loss: 2.134\n",
      "Iter 9581/10000 - Loss: 2.134\n",
      "Iter 9591/10000 - Loss: 2.134\n",
      "Iter 9601/10000 - Loss: 2.134\n",
      "Iter 9611/10000 - Loss: 2.134\n",
      "Iter 9621/10000 - Loss: 2.134\n",
      "Iter 9631/10000 - Loss: 2.134\n",
      "Iter 9641/10000 - Loss: 2.134\n",
      "Iter 9651/10000 - Loss: 2.134\n",
      "Iter 9661/10000 - Loss: 2.134\n",
      "Iter 9671/10000 - Loss: 2.134\n",
      "Iter 9681/10000 - Loss: 2.134\n",
      "Iter 9691/10000 - Loss: 2.134\n",
      "Iter 9701/10000 - Loss: 2.134\n",
      "Iter 9711/10000 - Loss: 2.134\n",
      "Iter 9721/10000 - Loss: 2.134\n",
      "Iter 9731/10000 - Loss: 2.134\n",
      "Iter 9741/10000 - Loss: 2.134\n",
      "Iter 9751/10000 - Loss: 2.134\n",
      "Iter 9761/10000 - Loss: 2.134\n",
      "Iter 9771/10000 - Loss: 2.134\n",
      "Iter 9781/10000 - Loss: 2.134\n",
      "Iter 9791/10000 - Loss: 2.134\n",
      "Iter 9801/10000 - Loss: 2.134\n",
      "Iter 9811/10000 - Loss: 2.134\n",
      "Iter 9821/10000 - Loss: 2.134\n",
      "Iter 9831/10000 - Loss: 2.134\n",
      "Iter 9841/10000 - Loss: 2.134\n",
      "Iter 9851/10000 - Loss: 2.134\n",
      "Iter 9861/10000 - Loss: 2.134\n",
      "Iter 9871/10000 - Loss: 2.134\n",
      "Iter 9881/10000 - Loss: 2.134\n",
      "Iter 9891/10000 - Loss: 2.134\n",
      "Iter 9901/10000 - Loss: 2.134\n",
      "Iter 9911/10000 - Loss: 2.134\n",
      "Iter 9921/10000 - Loss: 2.134\n",
      "Iter 9931/10000 - Loss: 2.134\n",
      "Iter 9941/10000 - Loss: 2.134\n",
      "Iter 9951/10000 - Loss: 2.134\n",
      "Iter 9961/10000 - Loss: 2.134\n",
      "Iter 9971/10000 - Loss: 2.134\n",
      "Iter 9981/10000 - Loss: 2.134\n",
      "Iter 9991/10000 - Loss: 2.134\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "modelHigh.train()\n",
    "likelihoodHigh.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizerHigh = torch.optim.Adam(modelHigh.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mllHigh = gpytorch.mlls.ExactMarginalLogLikelihood(likelihoodHigh, modelHigh)\n",
    "\n",
    "training_iter_high = 10000\n",
    "for i in range(training_iter_high):\n",
    "    optimizerHigh.zero_grad()\n",
    "    outputHigh = modelHigh(XHighNew)\n",
    "    lossHigh = -mllHigh(outputHigh, YHigh)\n",
    "    lossHigh.backward()\n",
    "    if i %10==0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter_high, lossHigh.item()))\n",
    "    optimizerHigh.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]\n",
      " [26.76227698]]\n",
      "0.2964664282893457\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "modelHigh.eval()\n",
    "likelihoodHigh.eval()\n",
    "\n",
    "# The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)\n",
    "# See https://arxiv.org/abs/1803.06058\n",
    "nsamples=1000\n",
    "Nts=9\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Make predictions\n",
    "    observed_pred1 = likelihoodLow(modelLow(XTest))\n",
    "    mean1=observed_pred1.mean\n",
    "    var1=observed_pred1.covariance_matrix\n",
    "    Z = np.random.multivariate_normal(mean1.flatten(),var1,nsamples)\n",
    "    \n",
    "    # push samples through f_2\n",
    "    tmp_m = np.zeros((nsamples,Nts))\n",
    "    tmp_v = np.zeros((nsamples,Nts))\n",
    "    XTest=XTest.reshape(-1,1)\n",
    "    \n",
    "    for i in range(0,nsamples):\n",
    "        XTest2=torch.from_numpy(np.vstack((XTest, Z[i,:][:,None]))).float().mean(axis=0).reshape(-1)\n",
    "        observed_pred2 =likelihoodHigh(modelHigh(XTest2))\n",
    "        mean2=observed_pred2.mean\n",
    "        var2=observed_pred2.covariance_matrix\n",
    "        tmp_m[i,:] = mean2.flatten()\n",
    "        tmp_v[i,:] = var2.flatten()\n",
    "\n",
    "    # get posterior mean and variance\n",
    "    mean = np.mean(tmp_m, axis = 0)[:,None]\n",
    "    var = np.mean(tmp_v, axis = 0)[:,None]+ np.var(tmp_m, axis = 0)[:,None]\n",
    "    var = np.abs(var)\n",
    "\n",
    "    error = np.linalg.norm(YTest - mean)/np.linalg.norm(YTest)\n",
    "     \n",
    "    print(mean)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (9, 1) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-588d20119beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Plot predictive means as blue line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved_pred2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# Shade between the lower and upper confidence bounds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tllok.n\\pycharmprojects\\gptour\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (9, 1) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize plot\n",
    "f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "# Get upper and lower confidence bounds\n",
    "lower, upper = observed_pred2.confidence_region()\n",
    "# Plot training data as black stars\n",
    "# ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "# Plot predictive means as blue line\n",
    "ax.plot(XTest.numpy(), observed_pred2.mean.numpy(), 'b')\n",
    "# Shade between the lower and upper confidence bounds\n",
    "ax.fill_between(XTest.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "ax.set_ylim([-5, 5])\n",
    "ax.legend(['Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
