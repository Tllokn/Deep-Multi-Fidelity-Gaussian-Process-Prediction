{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别对低保真度训练数据与高保真度训练数据建立神经网络进行特征提取，将低保真度数据特征与高保真度数据特征分别使用高斯过程进行预测。然后输入高保真度测试数据X，分别通过训练好的两个神经网络和高斯过程，再将两个正态分布相加，取出均值除以二，进行结果的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch.nn as nn\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32])\n"
     ]
    }
   ],
   "source": [
    "XTest = np.loadtxt('./dataset/x_test_high.txt').reshape(-1,32)\n",
    "# XTest = XTest.mean(axis=1).reshape(-1)\n",
    "YTest = np.loadtxt('./dataset/y_test_high.txt').reshape(-1)\n",
    "\n",
    "XLow = np.loadtxt('./dataset/x_train_low.txt').reshape(-1,32)\n",
    "# XLow = XLow.mean(axis=1).reshape(-1)\n",
    "YLow = (np.loadtxt('./dataset/y_train_low.txt')*1e4).reshape(-1)\n",
    "\n",
    "XHigh = np.loadtxt('./dataset/x_train_high.txt').reshape(-1,32)\n",
    "# XHigh = XHigh.mean(axis=1).reshape(-1)\n",
    "YHigh = np.loadtxt('./dataset/y_train_high.txt').reshape(-1)\n",
    "\n",
    "XLow = torch.from_numpy(XLow).float()\n",
    "YLow = torch.from_numpy(YLow).float()\n",
    "XHigh = torch.from_numpy(XHigh).float()\n",
    "YHigh = torch.from_numpy(YHigh).float()\n",
    "XTest = torch.from_numpy(XTest).float()\n",
    "YTest = torch.from_numpy(YTest).float()\n",
    "\n",
    "print(XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LowFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(32, 500))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(50, 2))\n",
    "        \n",
    "class HighFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(HighFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(32, 500))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(50, 2))\n",
    "\n",
    "featureExtractorLow = LowFeatureExtractor()\n",
    "featureExtractorHigh = HighFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowGPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(LowGPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "            self.feature_extractor = featureExtractorLow\n",
    "\n",
    "        def forward(self, train_x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(train_x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoodLow=gpytorch.likelihoods.GaussianLikelihood()\n",
    "modelLow=LowGPRegressionModel(XLow,YLow,likelihoodLow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighGPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(HighGPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "            self.feature_extractor = featureExtractorHigh\n",
    "\n",
    "        def forward(self, train_x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "#             LowMean=self.modelLow.mean\n",
    "#             x=torch.stack(train_x,LowMean)\n",
    "            projected_x = self.feature_extractor(x_train)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoodHigh=gpytorch.likelihoods.GaussianLikelihood()\n",
    "modelHigh=LowGPRegressionModel(XHigh,YHigh,likelihoodHigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 LossLow: 10.281410217285156\n",
      "Epoch 10 LossLow: 3.2677767276763916\n",
      "Epoch 20 LossLow: 2.512575387954712\n",
      "Epoch 30 LossLow: 2.1410915851593018\n",
      "Epoch 40 LossLow: 1.9278260469436646\n",
      "Epoch 50 LossLow: 1.7838828563690186\n",
      "Epoch 60 LossLow: 1.676425814628601\n",
      "Epoch 70 LossLow: 1.5927467346191406\n",
      "Epoch 80 LossLow: 1.5197761058807373\n",
      "Epoch 90 LossLow: 1.4382776021957397\n",
      "Epoch 100 LossLow: 1.3724088668823242\n",
      "Epoch 110 LossLow: 1.3190059661865234\n",
      "Epoch 120 LossLow: 1.262407660484314\n",
      "Epoch 130 LossLow: 1.2254501581192017\n",
      "Epoch 140 LossLow: 1.178896188735962\n",
      "Epoch 150 LossLow: 1.1489821672439575\n",
      "Epoch 160 LossLow: 1.0974420309066772\n",
      "Epoch 170 LossLow: 1.0636768341064453\n",
      "Epoch 180 LossLow: 1.0292953252792358\n",
      "Epoch 190 LossLow: 1.0120775699615479\n",
      "Epoch 200 LossLow: 0.9494389891624451\n",
      "Epoch 210 LossLow: 0.920555830001831\n",
      "Epoch 220 LossLow: 0.8863676190376282\n",
      "Epoch 230 LossLow: 0.8335289359092712\n",
      "Epoch 240 LossLow: 0.7931657433509827\n",
      "Epoch 250 LossLow: 0.7412766218185425\n",
      "Epoch 260 LossLow: 0.7197385430335999\n",
      "Epoch 270 LossLow: 0.6925143003463745\n",
      "Epoch 280 LossLow: 0.6458051800727844\n",
      "Epoch 290 LossLow: 0.6133731007575989\n",
      "Epoch 300 LossLow: 0.5848208069801331\n",
      "Epoch 310 LossLow: 0.5379383563995361\n",
      "Epoch 320 LossLow: 0.5156806707382202\n",
      "Epoch 330 LossLow: 0.4831176698207855\n",
      "Epoch 340 LossLow: 0.5112540125846863\n",
      "Epoch 350 LossLow: 0.40725550055503845\n",
      "Epoch 360 LossLow: 0.38069626688957214\n",
      "Epoch 370 LossLow: 0.3576541841030121\n",
      "Epoch 380 LossLow: 0.3086484372615814\n",
      "Epoch 390 LossLow: 0.2923980951309204\n",
      "Epoch 400 LossLow: 0.2523590624332428\n",
      "Epoch 410 LossLow: 0.2666669487953186\n",
      "Epoch 420 LossLow: 0.1797667294740677\n",
      "Epoch 430 LossLow: 0.16606983542442322\n",
      "Epoch 440 LossLow: 0.13516058027744293\n",
      "Epoch 450 LossLow: 0.16566121578216553\n",
      "Epoch 460 LossLow: 0.0994434654712677\n",
      "Epoch 470 LossLow: 0.02730325050652027\n",
      "Epoch 480 LossLow: -0.0055360510013997555\n",
      "Epoch 490 LossLow: 0.027188725769519806\n",
      "Epoch 500 LossLow: -0.01769322343170643\n",
      "Epoch 510 LossLow: -0.06518277525901794\n",
      "Epoch 520 LossLow: -0.009031961672008038\n",
      "Epoch 530 LossLow: -0.07331659644842148\n",
      "Epoch 540 LossLow: -0.07164916396141052\n",
      "Epoch 550 LossLow: -0.14706845581531525\n",
      "Epoch 560 LossLow: -0.1064683347940445\n",
      "Epoch 570 LossLow: -0.10197427123785019\n",
      "Epoch 580 LossLow: -0.1323644369840622\n",
      "Epoch 590 LossLow: -0.14078135788440704\n",
      "Epoch 600 LossLow: -0.11509979516267776\n",
      "Epoch 610 LossLow: -0.1617143154144287\n",
      "Epoch 620 LossLow: -0.1072143167257309\n",
      "Epoch 630 LossLow: -0.17089605331420898\n",
      "Epoch 640 LossLow: -0.1700982004404068\n",
      "Epoch 650 LossLow: -0.24651984870433807\n",
      "Epoch 660 LossLow: -0.3446978032588959\n",
      "Epoch 670 LossLow: -0.3413507044315338\n",
      "Epoch 680 LossLow: -0.3189961910247803\n",
      "Epoch 690 LossLow: -0.34531980752944946\n",
      "Epoch 700 LossLow: -0.3238845467567444\n",
      "Epoch 710 LossLow: -0.24778029322624207\n",
      "Epoch 720 LossLow: -0.3886956572532654\n",
      "Epoch 730 LossLow: -0.3432653546333313\n",
      "Epoch 740 LossLow: -0.4824534058570862\n",
      "Epoch 750 LossLow: -0.37046554684638977\n",
      "Epoch 760 LossLow: -0.4349261224269867\n",
      "Epoch 770 LossLow: -0.3441421091556549\n",
      "Epoch 780 LossLow: -0.21905231475830078\n",
      "Epoch 790 LossLow: -0.3031899929046631\n",
      "Epoch 800 LossLow: -0.37662407755851746\n",
      "Epoch 810 LossLow: -0.30953890085220337\n",
      "Epoch 820 LossLow: -0.43527600169181824\n",
      "Epoch 830 LossLow: -0.48756325244903564\n",
      "Epoch 840 LossLow: -0.4279610216617584\n",
      "Epoch 850 LossLow: -0.4973127245903015\n",
      "Epoch 860 LossLow: -0.4312152862548828\n",
      "Epoch 870 LossLow: -0.4523542821407318\n",
      "Epoch 880 LossLow: -0.3740430176258087\n",
      "Epoch 890 LossLow: -0.16862532496452332\n",
      "Epoch 900 LossLow: -0.3820359408855438\n",
      "Epoch 910 LossLow: -0.23765656352043152\n",
      "Epoch 920 LossLow: -0.3754565715789795\n",
      "Epoch 930 LossLow: -0.5045689344406128\n",
      "Epoch 940 LossLow: -0.4402276575565338\n",
      "Epoch 950 LossLow: -0.43920305371284485\n",
      "Epoch 960 LossLow: -0.5387917160987854\n",
      "Epoch 970 LossLow: -0.49835205078125\n",
      "Epoch 980 LossLow: -0.5574026703834534\n",
      "Epoch 990 LossLow: -0.6000275611877441\n",
      "Epoch 0 LossHigh: 65.47769165039062\n",
      "Epoch 10 LossHigh: 17.44808578491211\n",
      "Epoch 20 LossHigh: 13.078981399536133\n",
      "Epoch 30 LossHigh: 11.060797691345215\n",
      "Epoch 40 LossHigh: 9.822182655334473\n",
      "Epoch 50 LossHigh: 8.951679229736328\n",
      "Epoch 60 LossHigh: 8.303398132324219\n",
      "Epoch 70 LossHigh: 7.811933994293213\n",
      "Epoch 80 LossHigh: 7.433106899261475\n",
      "Epoch 90 LossHigh: 7.127997398376465\n",
      "Epoch 100 LossHigh: 6.871645450592041\n",
      "Epoch 110 LossHigh: 6.65036678314209\n",
      "Epoch 120 LossHigh: 6.456148624420166\n",
      "Epoch 130 LossHigh: 6.2836432456970215\n",
      "Epoch 140 LossHigh: 6.128897666931152\n",
      "Epoch 150 LossHigh: 5.989002704620361\n",
      "Epoch 160 LossHigh: 5.861719131469727\n",
      "Epoch 170 LossHigh: 5.745235443115234\n",
      "Epoch 180 LossHigh: 5.63816499710083\n",
      "Epoch 190 LossHigh: 5.539247989654541\n",
      "Epoch 200 LossHigh: 5.447475433349609\n",
      "Epoch 210 LossHigh: 5.361992359161377\n",
      "Epoch 220 LossHigh: 5.282079696655273\n",
      "Epoch 230 LossHigh: 5.207120418548584\n",
      "Epoch 240 LossHigh: 5.1366400718688965\n",
      "Epoch 250 LossHigh: 5.070151329040527\n",
      "Epoch 260 LossHigh: 5.007222652435303\n",
      "Epoch 270 LossHigh: 4.947508811950684\n",
      "Epoch 280 LossHigh: 4.890713691711426\n",
      "Epoch 290 LossHigh: 4.836574077606201\n",
      "Epoch 300 LossHigh: 4.784938812255859\n",
      "Epoch 310 LossHigh: 4.735524654388428\n",
      "Epoch 320 LossHigh: 4.688133239746094\n",
      "Epoch 330 LossHigh: 4.642607688903809\n",
      "Epoch 340 LossHigh: 4.598804473876953\n",
      "Epoch 350 LossHigh: 4.556596755981445\n",
      "Epoch 360 LossHigh: 4.515870094299316\n",
      "Epoch 370 LossHigh: 4.476521015167236\n",
      "Epoch 380 LossHigh: 4.43845272064209\n",
      "Epoch 390 LossHigh: 4.401556015014648\n",
      "Epoch 400 LossHigh: 4.365747451782227\n",
      "Epoch 410 LossHigh: 4.330989837646484\n",
      "Epoch 420 LossHigh: 4.297220230102539\n",
      "Epoch 430 LossHigh: 4.264376163482666\n",
      "Epoch 440 LossHigh: 4.2324042320251465\n",
      "Epoch 450 LossHigh: 4.201277256011963\n",
      "Epoch 460 LossHigh: 4.170914649963379\n",
      "Epoch 470 LossHigh: 4.1413140296936035\n",
      "Epoch 480 LossHigh: 4.112396240234375\n",
      "Epoch 490 LossHigh: 4.084120273590088\n",
      "Epoch 500 LossHigh: 4.056509494781494\n",
      "Epoch 510 LossHigh: 4.029454708099365\n",
      "Epoch 520 LossHigh: 4.002913951873779\n",
      "Epoch 530 LossHigh: 3.976945161819458\n",
      "Epoch 540 LossHigh: 3.951462984085083\n",
      "Epoch 550 LossHigh: 3.9264442920684814\n",
      "Epoch 560 LossHigh: 3.9018497467041016\n",
      "Epoch 570 LossHigh: 3.8777904510498047\n",
      "Epoch 580 LossHigh: 3.8539950847625732\n",
      "Epoch 590 LossHigh: 3.8306095600128174\n",
      "Epoch 600 LossHigh: 3.8076908588409424\n",
      "Epoch 610 LossHigh: 3.784984827041626\n",
      "Epoch 620 LossHigh: 3.7626426219940186\n",
      "Epoch 630 LossHigh: 3.7407100200653076\n",
      "Epoch 640 LossHigh: 3.718945026397705\n",
      "Epoch 650 LossHigh: 3.6974985599517822\n",
      "Epoch 660 LossHigh: 3.676361560821533\n",
      "Epoch 670 LossHigh: 3.655421733856201\n",
      "Epoch 680 LossHigh: 3.634645700454712\n",
      "Epoch 690 LossHigh: 3.614112377166748\n",
      "Epoch 700 LossHigh: 3.593858003616333\n",
      "Epoch 710 LossHigh: 3.57380747795105\n",
      "Epoch 720 LossHigh: 3.5539495944976807\n",
      "Epoch 730 LossHigh: 3.534311532974243\n",
      "Epoch 740 LossHigh: 3.5148441791534424\n",
      "Epoch 750 LossHigh: 3.4956021308898926\n",
      "Epoch 760 LossHigh: 3.476473093032837\n",
      "Epoch 770 LossHigh: 3.4575507640838623\n",
      "Epoch 780 LossHigh: 3.4387776851654053\n",
      "Epoch 790 LossHigh: 3.4201159477233887\n",
      "Epoch 800 LossHigh: 3.40162992477417\n",
      "Epoch 810 LossHigh: 3.3832621574401855\n",
      "Epoch 820 LossHigh: 3.3650455474853516\n",
      "Epoch 830 LossHigh: 3.3469271659851074\n",
      "Epoch 840 LossHigh: 3.328921318054199\n",
      "Epoch 850 LossHigh: 3.3110218048095703\n",
      "Epoch 860 LossHigh: 3.2932164669036865\n",
      "Epoch 870 LossHigh: 3.2755022048950195\n",
      "Epoch 880 LossHigh: 3.257861852645874\n",
      "Epoch 890 LossHigh: 3.2403335571289062\n",
      "Epoch 900 LossHigh: 3.2228879928588867\n",
      "Epoch 910 LossHigh: 3.205512523651123\n",
      "Epoch 920 LossHigh: 3.188199281692505\n",
      "Epoch 930 LossHigh: 3.170938491821289\n",
      "Epoch 940 LossHigh: 3.1537282466888428\n",
      "Epoch 950 LossHigh: 3.136557102203369\n",
      "Epoch 960 LossHigh: 3.1194229125976562\n",
      "Epoch 970 LossHigh: 3.10231351852417\n",
      "Epoch 980 LossHigh: 3.0852243900299072\n",
      "Epoch 990 LossHigh: 3.068146228790283\n",
      "Epoch 1000 LossHigh: 3.0510692596435547\n",
      "Epoch 1010 LossHigh: 3.033984661102295\n",
      "Epoch 1020 LossHigh: 3.016881227493286\n",
      "Epoch 1030 LossHigh: 2.9997501373291016\n",
      "Epoch 1040 LossHigh: 2.9825856685638428\n",
      "Epoch 1050 LossHigh: 2.9653797149658203\n",
      "Epoch 1060 LossHigh: 2.948122262954712\n",
      "Epoch 1070 LossHigh: 2.9308078289031982\n",
      "Epoch 1080 LossHigh: 2.9134325981140137\n",
      "Epoch 1090 LossHigh: 2.8959946632385254\n",
      "Epoch 1100 LossHigh: 2.8784971237182617\n",
      "Epoch 1110 LossHigh: 2.860930919647217\n",
      "Epoch 1120 LossHigh: 2.8433048725128174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1130 LossHigh: 2.8256289958953857\n",
      "Epoch 1140 LossHigh: 2.8079099655151367\n",
      "Epoch 1150 LossHigh: 2.7901546955108643\n",
      "Epoch 1160 LossHigh: 2.7723653316497803\n",
      "Epoch 1170 LossHigh: 2.7545506954193115\n",
      "Epoch 1180 LossHigh: 2.736710786819458\n",
      "Epoch 1190 LossHigh: 2.718851327896118\n",
      "Epoch 1200 LossHigh: 2.7009730339050293\n",
      "Epoch 1210 LossHigh: 2.683077812194824\n",
      "Epoch 1220 LossHigh: 2.6651647090911865\n",
      "Epoch 1230 LossHigh: 2.6472370624542236\n",
      "Epoch 1240 LossHigh: 2.6292924880981445\n",
      "Epoch 1250 LossHigh: 2.6113288402557373\n",
      "Epoch 1260 LossHigh: 2.593348979949951\n",
      "Epoch 1270 LossHigh: 2.5753490924835205\n",
      "Epoch 1280 LossHigh: 2.557326555252075\n",
      "Epoch 1290 LossHigh: 2.5392818450927734\n",
      "Epoch 1300 LossHigh: 2.5212268829345703\n",
      "Epoch 1310 LossHigh: 2.503143310546875\n",
      "Epoch 1320 LossHigh: 2.4850306510925293\n",
      "Epoch 1330 LossHigh: 2.4668822288513184\n",
      "Epoch 1340 LossHigh: 2.4486985206604004\n",
      "Epoch 1350 LossHigh: 2.4304745197296143\n",
      "Epoch 1360 LossHigh: 2.412208080291748\n",
      "Epoch 1370 LossHigh: 2.393895387649536\n",
      "Epoch 1380 LossHigh: 2.3755366802215576\n",
      "Epoch 1390 LossHigh: 2.35713267326355\n",
      "Epoch 1400 LossHigh: 2.3386759757995605\n",
      "Epoch 1410 LossHigh: 2.3201730251312256\n",
      "Epoch 1420 LossHigh: 2.301625967025757\n",
      "Epoch 1430 LossHigh: 2.2830326557159424\n",
      "Epoch 1440 LossHigh: 2.2644004821777344\n",
      "Epoch 1450 LossHigh: 2.245728015899658\n",
      "Epoch 1460 LossHigh: 2.227088451385498\n",
      "Epoch 1470 LossHigh: 2.2411601543426514\n",
      "Epoch 1480 LossHigh: 2.2630503177642822\n",
      "Epoch 1490 LossHigh: 2.2051239013671875\n",
      "Epoch 1500 LossHigh: 2.18843412399292\n",
      "Epoch 1510 LossHigh: 2.176647424697876\n",
      "Epoch 1520 LossHigh: 2.157862424850464\n",
      "Epoch 1530 LossHigh: 2.1375722885131836\n",
      "Epoch 1540 LossHigh: 2.1201817989349365\n",
      "Epoch 1550 LossHigh: 2.1038498878479004\n",
      "Epoch 1560 LossHigh: 2.0866990089416504\n",
      "Epoch 1570 LossHigh: 2.0688395500183105\n",
      "Epoch 1580 LossHigh: 2.0512959957122803\n",
      "Epoch 1590 LossHigh: 2.03416109085083\n",
      "Epoch 1600 LossHigh: 2.016850471496582\n",
      "Epoch 1610 LossHigh: 1.9997729063034058\n",
      "Epoch 1620 LossHigh: 1.9824825525283813\n",
      "Epoch 1630 LossHigh: 1.9652677774429321\n",
      "Epoch 1640 LossHigh: 1.9481236934661865\n",
      "Epoch 1650 LossHigh: 1.9302971363067627\n",
      "Epoch 1660 LossHigh: 1.9136956930160522\n",
      "Epoch 1670 LossHigh: 1.897333025932312\n",
      "Epoch 1680 LossHigh: 1.8797898292541504\n",
      "Epoch 1690 LossHigh: 1.8625887632369995\n",
      "Epoch 1700 LossHigh: 1.845734715461731\n",
      "Epoch 1710 LossHigh: 1.8292624950408936\n",
      "Epoch 1720 LossHigh: 1.812626600265503\n",
      "Epoch 1730 LossHigh: 1.7956969738006592\n",
      "Epoch 1740 LossHigh: 1.7787599563598633\n",
      "Epoch 1750 LossHigh: 1.7629293203353882\n",
      "Epoch 1760 LossHigh: 1.7469892501831055\n",
      "Epoch 1770 LossHigh: 1.7305370569229126\n",
      "Epoch 1780 LossHigh: 1.7150156497955322\n",
      "Epoch 1790 LossHigh: 1.6994476318359375\n",
      "Epoch 1800 LossHigh: 1.6844520568847656\n",
      "Epoch 1810 LossHigh: 1.6698004007339478\n",
      "Epoch 1820 LossHigh: 1.655428171157837\n",
      "Epoch 1830 LossHigh: 1.6427278518676758\n",
      "Epoch 1840 LossHigh: 1.630484938621521\n",
      "Epoch 1850 LossHigh: 1.6192657947540283\n",
      "Epoch 1860 LossHigh: 1.6106665134429932\n",
      "Epoch 1870 LossHigh: 1.6037565469741821\n",
      "Epoch 1880 LossHigh: 1.5996274948120117\n",
      "Epoch 1890 LossHigh: 1.5965909957885742\n",
      "Epoch 1900 LossHigh: 1.5941630601882935\n",
      "Epoch 1910 LossHigh: 1.5918408632278442\n",
      "Epoch 1920 LossHigh: 1.5874837636947632\n",
      "Epoch 1930 LossHigh: 1.5813809633255005\n",
      "Epoch 1940 LossHigh: 1.5741780996322632\n",
      "Epoch 1950 LossHigh: 1.565258264541626\n",
      "Epoch 1960 LossHigh: 1.5531026124954224\n",
      "Epoch 1970 LossHigh: 1.552101492881775\n",
      "Epoch 1980 LossHigh: 1.5603325366973877\n",
      "Epoch 1990 LossHigh: 1.5535823106765747\n",
      "Epoch 2000 LossHigh: 1.473343849182129\n",
      "Epoch 2010 LossHigh: 1.4267499446868896\n",
      "Epoch 2020 LossHigh: 1.4749571084976196\n",
      "Epoch 2030 LossHigh: 1.6426805257797241\n",
      "Epoch 2040 LossHigh: 1.5200811624526978\n",
      "Epoch 2050 LossHigh: 1.3951202630996704\n",
      "Epoch 2060 LossHigh: 1.3965787887573242\n",
      "Epoch 2070 LossHigh: 1.4774656295776367\n",
      "Epoch 2080 LossHigh: 1.4748982191085815\n",
      "Epoch 2090 LossHigh: 1.4401881694793701\n",
      "Epoch 2100 LossHigh: 1.4605122804641724\n",
      "Epoch 2110 LossHigh: 1.5022871494293213\n",
      "Epoch 2120 LossHigh: 1.3793716430664062\n",
      "Epoch 2130 LossHigh: 1.335763692855835\n",
      "Epoch 2140 LossHigh: 1.4617005586624146\n",
      "Epoch 2150 LossHigh: 1.4107650518417358\n",
      "Epoch 2160 LossHigh: 1.3605176210403442\n",
      "Epoch 2170 LossHigh: 1.3858439922332764\n",
      "Epoch 2180 LossHigh: 1.28895103931427\n",
      "Epoch 2190 LossHigh: 1.2673768997192383\n",
      "Epoch 2200 LossHigh: 1.3960007429122925\n",
      "Epoch 2210 LossHigh: 1.4554917812347412\n",
      "Epoch 2220 LossHigh: 1.4022489786148071\n",
      "Epoch 2230 LossHigh: 1.3286546468734741\n",
      "Epoch 2240 LossHigh: 1.2577577829360962\n",
      "Epoch 2250 LossHigh: 1.2572237253189087\n",
      "Epoch 2260 LossHigh: 1.3375489711761475\n",
      "Epoch 2270 LossHigh: 1.3617674112319946\n",
      "Epoch 2280 LossHigh: 1.2984174489974976\n",
      "Epoch 2290 LossHigh: 1.32988703250885\n",
      "Epoch 2300 LossHigh: 1.272923231124878\n",
      "Epoch 2310 LossHigh: 1.1550087928771973\n",
      "Epoch 2320 LossHigh: 1.1697580814361572\n",
      "Epoch 2330 LossHigh: 1.3041373491287231\n",
      "Epoch 2340 LossHigh: 1.3276407718658447\n",
      "Epoch 2350 LossHigh: 1.3144495487213135\n",
      "Epoch 2360 LossHigh: 1.305789828300476\n",
      "Epoch 2370 LossHigh: 1.1823153495788574\n",
      "Epoch 2380 LossHigh: 1.1250723600387573\n",
      "Epoch 2390 LossHigh: 1.1756558418273926\n",
      "Epoch 2400 LossHigh: 1.279388189315796\n",
      "Epoch 2410 LossHigh: 1.2467371225357056\n",
      "Epoch 2420 LossHigh: 1.1827462911605835\n",
      "Epoch 2430 LossHigh: 1.228652834892273\n",
      "Epoch 2440 LossHigh: 1.1733735799789429\n",
      "Epoch 2450 LossHigh: 1.0582475662231445\n",
      "Epoch 2460 LossHigh: 1.058817982673645\n",
      "Epoch 2470 LossHigh: 1.1377527713775635\n",
      "Epoch 2480 LossHigh: 1.204207420349121\n",
      "Epoch 2490 LossHigh: 1.2852777242660522\n",
      "Epoch 2500 LossHigh: 1.2376593351364136\n",
      "Epoch 2510 LossHigh: 1.0952999591827393\n",
      "Epoch 2520 LossHigh: 1.0428060293197632\n",
      "Epoch 2530 LossHigh: 1.0752671957015991\n",
      "Epoch 2540 LossHigh: 1.124068021774292\n",
      "Epoch 2550 LossHigh: 1.1159826517105103\n",
      "Epoch 2560 LossHigh: 1.1030988693237305\n",
      "Epoch 2570 LossHigh: 0.9064317345619202\n",
      "Epoch 2580 LossHigh: 1.052879810333252\n",
      "Epoch 2590 LossHigh: 1.25648832321167\n",
      "Epoch 2600 LossHigh: 0.8944038152694702\n",
      "Epoch 2610 LossHigh: 0.8657541871070862\n",
      "Epoch 2620 LossHigh: 1.1237356662750244\n",
      "Epoch 2630 LossHigh: 1.1653804779052734\n",
      "Epoch 2640 LossHigh: 0.9062727689743042\n",
      "Epoch 2650 LossHigh: 0.9117986559867859\n",
      "Epoch 2660 LossHigh: 1.0575660467147827\n",
      "Epoch 2670 LossHigh: 1.0267072916030884\n",
      "Epoch 2680 LossHigh: 0.9676621556282043\n",
      "Epoch 2690 LossHigh: 0.9439171552658081\n",
      "Epoch 2700 LossHigh: 0.9298007488250732\n",
      "Epoch 2710 LossHigh: 0.9247444868087769\n",
      "Epoch 2720 LossHigh: 0.9249493479728699\n",
      "Epoch 2730 LossHigh: 0.9221068620681763\n",
      "Epoch 2740 LossHigh: 0.9109163880348206\n",
      "Epoch 2750 LossHigh: 0.9176360368728638\n",
      "Epoch 2760 LossHigh: 0.939589262008667\n",
      "Epoch 2770 LossHigh: 0.9946660399436951\n",
      "Epoch 2780 LossHigh: 1.0435227155685425\n",
      "Epoch 2790 LossHigh: 0.8107409477233887\n",
      "Epoch 2800 LossHigh: 0.7690394520759583\n",
      "Epoch 2810 LossHigh: 0.9397669434547424\n",
      "Epoch 2820 LossHigh: 0.9786952137947083\n",
      "Epoch 2830 LossHigh: 0.8746442198753357\n",
      "Epoch 2840 LossHigh: 0.8887342810630798\n",
      "Epoch 2850 LossHigh: 0.9285873174667358\n",
      "Epoch 2860 LossHigh: 0.9001823663711548\n",
      "Epoch 2870 LossHigh: 0.8636811375617981\n",
      "Epoch 2880 LossHigh: 0.891435444355011\n",
      "Epoch 2890 LossHigh: 0.9067757725715637\n",
      "Epoch 2900 LossHigh: 0.7914931178092957\n",
      "Epoch 2910 LossHigh: 0.7699578404426575\n",
      "Epoch 2920 LossHigh: 0.8570361137390137\n",
      "Epoch 2930 LossHigh: 0.862708568572998\n",
      "Epoch 2940 LossHigh: 0.8091462254524231\n",
      "Epoch 2950 LossHigh: 0.7774773836135864\n",
      "Epoch 2960 LossHigh: 0.7898998260498047\n",
      "Epoch 2970 LossHigh: 0.8019442558288574\n",
      "Epoch 2980 LossHigh: 0.789052426815033\n",
      "Epoch 2990 LossHigh: 0.7395763397216797\n"
     ]
    }
   ],
   "source": [
    "modelHigh.train()\n",
    "likelihoodHigh.train()\n",
    "modelLow.train()\n",
    "likelihoodLow.train()\n",
    "\n",
    "optimizerLow=torch.optim.Adam([\n",
    "    {'params':modelLow.mean_module.parameters()},\n",
    "    {'params':modelLow.covar_module.parameters()},\n",
    "    {'params':modelLow.feature_extractor.parameters()},\n",
    "    {'params':modelLow.likelihood.parameters()},\n",
    "],lr=0.01)\n",
    "optimizerHigh=torch.optim.SGD([\n",
    "    {'params':modelHigh.mean_module.parameters()},\n",
    "    {'params':modelHigh.covar_module.parameters()},\n",
    "    {'params':modelHigh.feature_extractor.parameters()},\n",
    "    {'params':modelHigh.likelihood.parameters()},\n",
    "],lr=0.01)\n",
    "\n",
    "mllLow=gpytorch.mlls.ExactMarginalLogLikelihood(likelihoodLow,modelLow)\n",
    "mllHigh=gpytorch.mlls.ExactMarginalLogLikelihood(likelihoodHigh,modelHigh)\n",
    "\n",
    "for t in range(1000):\n",
    "    optimizerLow.zero_grad()    \n",
    "    outputLow=modelLow(XLow)    \n",
    "    lossLow=-mllLow(outputLow,YLow)    \n",
    "    if t%10==0:\n",
    "        print(\"Epoch\",t,\"LossLow:\",lossLow.item())  \n",
    "    lossLow.backward()\n",
    "    optimizerLow.step()\n",
    "\n",
    "\n",
    "for t in range(3000):\n",
    "    optimizerHigh.zero_grad()\n",
    "    outputHigh=modelHigh(XHigh)   \n",
    "    lossHigh=-mllHigh(outputHigh,YHigh)\n",
    "    if t%10==0:\n",
    "        print(\"Epoch\",t,\"LossHigh:\",lossHigh.item())   \n",
    "    lossHigh.backward()\n",
    "    optimizerHigh.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32])\n"
     ]
    }
   ],
   "source": [
    "modelLow.eval()\n",
    "likelihoodLow.eval()\n",
    "modelHigh.eval()\n",
    "likelihoodHigh.eval()\n",
    "''' Predict at test points '''\n",
    "# sample f_1 at xtest\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predLow = likelihoodLow(modelLow(XTest))\n",
    "    predHigh=likelihoodHigh(modelHigh(XTest))\n",
    "    \n",
    "    pred=predLow+predHigh\n",
    "    print(XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score -3.2244657432256068\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = pred.confidence_region()\n",
    "    \n",
    "    X=np.array(XTest).mean(axis=1).reshape(-1)\n",
    "    YTest=np.array(YTest).reshape(-1)\n",
    "    lower=lower.numpy().reshape(-1)\n",
    "    upper=upper.numpy().reshape(-1)\n",
    "    mean=((pred.mean-3)/2).numpy().reshape(-1)\n",
    "    \n",
    "    zipped = zip(X,YTest, mean,lower,upper)\n",
    "    sort_zipped = sorted(zipped, key=lambda x: (x[0], x[1],x[2],x[3]))\n",
    "    result = zip(*sort_zipped)\n",
    "    X,YTest,mean,lower,upper = [list(x) for x in result]\n",
    "    \n",
    "    lowerlist=((np.array(lower)-3)/2).tolist() \n",
    "    upperlist=((np.array(upper)-3)/2).tolist()\n",
    "    \n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(X,YTest,'r')\n",
    "    ax.plot(X,mean,'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "#     ax.fill_between(X,lowerlist,upperlist, alpha=0.5)\n",
    "#     ax.set_ylim([-3, 3])\n",
    "    ax.legend([ 'Exact','Mean'])\n",
    "    print(\"r2 score\",r2_score(mean,YTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
